description = 'Spring for Apache Hadoop'

defaultTasks 'build'

apply plugin: 'eclipse'
apply plugin: 'idea'

buildscript {
	repositories {
		maven { url "http://repo.springsource.org/plugins-release" }
	}
	dependencies {
		classpath("org.springframework.build.gradle:propdeps-plugin:0.0.3")
		classpath("org.springframework.build.gradle:docbook-reference-plugin:0.2.6")
	}
}

allprojects {
	group = 'org.springframework.data'

	repositories {
		mavenCentral()
		maven { url 'http://repo.springsource.org/libs-milestone' }
	}
}

def javaProjects() {
	subprojects.findAll { project -> project.name != 'docs' }
}

def hadoopProjects() {
	subprojects.findAll { project -> project.name.contains('-hadoop-') }
}

// 
//  Select the Hadoop distribution used for building the binaries
// 
def List hadoop = []

def hadoopDefault = "hadoop10"
def hadoopDistro = project.hasProperty("distro") ? project.getProperty("distro") : hadoopDefault
def hadoopVersion = hadoop10Version

// make it possible to use Pig jars compiled for Hadoop 2.0
def pigQualifier = ''

// handle older Hive version
def hiveGroup = "org.apache.hive"

// default is Hadoop 1.0.x
switch (hadoopDistro) {

	// Cloudera CDH3
	case "cdh3":
		hadoopVersion = cdh3Version
		println "Using Cloudera CDH3 [$hadoopVersion]"
		hbaseVersion = cdh3HbaseVersion
		// Hive in CDH3 is too old and does not allow Hive Server to be compiled
		// Note that the POMs and repo are incomplete (hive-builtin is missing)
		// hiveVersion = cdh3HiveVersion
		// hiveGroup = "org.apache.hadoop.hive"
		pigVersion = cdh3PigVersion
		break;	

	// Cloudera CDH4
	case "cdh4":
		hadoopVersion = cdh4MR1Version
		println "Using Cloudera CDH4 [$hadoopVersion]"
		hbaseVersion = cdh4HbaseVersion
		hiveVersion = cdh4HiveVersion
		pigVersion = cdh4PigVersion
		thriftVersion = cdh4ThriftVersion
		break;	

	// Pivotal HD 1.0
	case "phd1":
		hadoopVersion = phd1Version
		println "Using Pivotal HD 1.0 - [$hadoopVersion]"
		hbaseVersion = phd1HbaseVersion
		hiveVersion = phd1HiveVersion
		pigVersion = phd1PigVersion
		thriftVersion = phd1ThriftVersion
		hadoop = ["org.apache.hadoop:hadoop-common:$hadoopVersion",
			  "org.apache.hadoop:hadoop-hdfs:$hadoopVersion",
			  "org.apache.hadoop:hadoop-mapreduce-client-core:$hadoopVersion",
			  "org.apache.hadoop:hadoop-streaming:$hadoopVersion",
			  "org.apache.hadoop:hadoop-distcp:$hadoopVersion"]
		break;

	// Hortonworks Data Platform 1.3
	case "hdp13":
		hadoopVersion = hdp13Version
		println "Using Hortonworks Data Platform 1.3 [$hadoopVersion]"
		hadoop = ["org.apache.hadoop:hadoop-streaming:$hadoopVersion",
			"org.apache.hadoop:hadoop-tools:$hadoopVersion"]
		hbaseVersion = hdp13HbaseVersion
		hiveVersion = hdp13HiveVersion
		pigVersion = hdp13PigVersion
		thriftVersion = hdp13ThriftVersion
		break;

	// Hadoop 2.0 Alpha
	case "hadoop20":
		hadoopVersion = hd20Version
		println "Using Apache Hadoop 2.0 - [$hadoopVersion]"
		hadoop = ["org.apache.hadoop:hadoop-common:$hadoopVersion",
			  "org.apache.hadoop:hadoop-hdfs:$hadoopVersion",
			  "org.apache.hadoop:hadoop-mapreduce-client-core:$hadoopVersion",
			  "org.apache.hadoop:hadoop-streaming:$hadoopVersion",
			  "org.apache.hadoop:hadoop-distcp:$hadoopVersion"]
		hbaseVersion = hd20HbaseVersion
		hiveVersion = hd20HiveVersion
		pigVersion = hd20PigVersion
		pigQualifier = ':h2'
		thriftVersion = hd20ThriftVersion
		break;

	// Hadoop 1.2.x Beta
	case "hadoop12":
		hadoopVersion = hd12Version
		println "Using Apache Hadoop 1.2.x [$hadoopVersion]"
		hadoop = ["org.apache.hadoop:hadoop-streaming:$hadoopVersion",
			"org.apache.hadoop:hadoop-tools:$hadoopVersion"]
		hbaseVersion = hd12HbaseVersion
		hiveVersion = hd12HiveVersion
		pigVersion = hd12PigVersion
		thriftVersion = hd12ThriftVersion
		break;

	// Hadoop 1.1.x
	case "hadoop11":
		hadoopVersion = hadoop11Version
		println "Using Apache Hadoop 1.1.x [$hadoopVersion]"
		hadoop = ["org.apache.hadoop:hadoop-streaming:$hadoopVersion",
			  "org.apache.hadoop:hadoop-tools:$hadoopVersion"]
		break;

	default:
		if (!project.hasProperty("distro")) {
			println "Using default distro: Apache Hadoop [$hadoopVersion]"
		} else {
			if (hadoopDistro == hadoopDefault) {
				println "Using Apache Hadoop 1.0.x [$hadoopVersion]"
			} else {
				println "$hadoopDistro is not a supported distro; using default distro: Apache Hadoop [$hadoopVersion]"
			}
		}
		hadoopVersion = hadoop10Version
		hadoop = ["org.apache.hadoop:hadoop-streaming:$hadoopVersion",
			  "org.apache.hadoop:hadoop-tools:$hadoopVersion"]
}

configure(javaProjects()) {

	apply plugin: 'java'
	apply from:   "${rootProject.projectDir}/maven.gradle"
	apply plugin: 'eclipse'
	apply plugin: 'idea'
	apply plugin: 'propdeps'
	apply plugin: 'propdeps-idea'
	apply plugin: 'propdeps-eclipse'

	sourceCompatibility=1.6
	targetCompatibility=1.6

	// assume we are skipping these tests (must be enabled explicitly)
	ext.skipPig = true
	ext.skipHive = true
	ext.skipHBase = true
	ext.skipWebHdfs = true
	ext.skipCascading = true

	// exclude poms from the classpath (pulled in by Cloudera)
	eclipse.classpath.file {
		whenMerged { classpath ->
			classpath.entries.removeAll { entry -> entry.toString().contains(".pom") }
		}
	}

	eclipse {
		project {
			natures += 'org.springframework.ide.eclipse.core.springnature'
		}
	}

	// dependencies that are common across all java projects
	dependencies {
		compile "org.springframework:spring-aop:$springVersion"
		compile "org.springframework:spring-context:$springVersion"
		compile "org.springframework:spring-context-support:$springVersion"
		compile "org.springframework:spring-jdbc:$springVersion"
		compile "org.springframework:spring-tx:$springVersion"
	}

	task sourcesJar(type: Jar) {
		classifier = 'sources'
		from sourceSets.main.allJava
	}

	task javadocJar(type: Jar) {
		classifier = 'javadoc'
		from javadoc
	}

	artifacts {
		archives sourcesJar
		archives javadocJar
	}

	assemble.dependsOn = ['jar', 'sourcesJar']

	javadoc {
		ext.srcDir = file("${projectDir}/docs/src/api")
	  
		configure(options) {
			stylesheetFile = file("${rootProject.projectDir}/docs/src/api/spring-javadoc.css")
			overview = "${rootProject.projectDir}/docs/src/api/overview.html"
			docFilesSubDirs = true
			outputLevel = org.gradle.external.javadoc.JavadocOutputLevel.QUIET
			breakIterator = true
			author = true
			showFromProtected()

	//		groups = [
	//		'Spring Data Hadoop' : ['org.springframework.data.hadoop*'],
	//		]
  
			links = [
				"http://static.springframework.org/spring/docs/3.0.x/javadoc-api",
				"http://download.oracle.com/javase/6/docs/api",
				"http://commons.apache.org/proper/commons-logging/commons-logging-1.1.1/apidocs/",
				"http://logging.apache.org/log4j/1.2/apidocs/",
				"http://hadoop.apache.org/common/docs/current/api/",
				"http://hbase.apache.org/apidocs/",
				"http://pig.apache.org/docs/r0.10.0/api/",
				"http://hive.apache.org/docs/r0.7.1/api/",
				"http://static.springsource.org/spring-batch/apidocs/",
				"http://static.springsource.org/spring-integration/api/",
				"https://builds.apache.org/job/Thrift/javadoc/",
				"http://docs.cascading.org/cascading/2.1/javadoc/"
			]

			exclude "org/springframework/data/hadoop/config/**"
	  	}

		title = "${rootProject.description} ${version} API"
	}

	jar {
		manifest.attributes["Created-By"] = "${System.getProperty("java.version")} (${System.getProperty("java.specification.vendor")})"
		manifest.attributes['Implementation-Title'] = 'spring-data-hadoop'
		manifest.attributes['Implementation-Version'] = project.version
		manifest.attributes['Implementation-URL'] = "http://www.springsource.org/spring-data/hadoop"
		manifest.attributes['Implementation-Vendor'] = "SpringSource"
		manifest.attributes['Implementation-Vendor-Id'] = "org.springframework"

		def build = System.env['SHDP.BUILD']
		if (build != null)
			manifest.attributes['Build'] = build

		String rev = "unknown"

		// parse the git files to find out the revision
		File gitHead = file('.git/HEAD')
		if (gitHead.exists()) {
			gitHead = file('.git/' + gitHead.text.trim().replace('ref: ',''))
			if (gitHead.exists()) { rev = gitHead.text }
		}

		from("$rootDir/docs/src/info") {
			include "license.txt"
			include "notice.txt"
			into "META-INF"
			expand(copyright: new Date().format('yyyy'), version: project.version)
		}

		manifest.attributes['Repository-Revision'] = rev
	}

}

configure(hadoopProjects()) {

	// default is Hadoop 1.0.x
	switch (hadoopDistro) {

		// Cloudera CDH3
		case "cdh3":
			dependencies {
				optional("org.apache.hadoop:hadoop-streaming:$hadoopVersion") 
				optional("org.apache.hadoop:hadoop-tools:$hadoopVersion")
				testCompile "org.apache.hadoop:hadoop-examples:$hadoopVersion"
			}
			break;	

		// Cloudera CDH4
		case "cdh4":
			dependencies {
				optional("org.apache.hadoop:hadoop-streaming:$cdh4MR1Version") 
				optional("org.apache.hadoop:hadoop-tools:$cdh4MR1Version")
				optional("org.apache.hadoop:hadoop-common:$cdh4Version")
				optional("org.apache.hadoop:hadoop-hdfs:$cdh4Version")
				testCompile "org.apache.hadoop:hadoop-examples:$hadoopVersion"
			}
			break;	

		// Pivotal HD 2.0
		case "phd1":
			dependencies {
				compile "org.apache.hive:hive-common:$hiveVersion"
				compile "org.apache.hive:hive-metastore:$hiveVersion"
				compile "org.apache.hive:hive-exec:$hiveVersion"
				testCompile "org.apache.hadoop:hadoop-mapreduce-examples:$hadoopVersion"
				testRuntime "org.apache.hadoop:hadoop-mapreduce-client-jobclient:$hadoopVersion"
			}
			break;

		// Hortonworks Data Platform 1.3
		case "hdp13":
			dependencies {
				testCompile "org.apache.hadoop:hadoop-examples:$hadoopVersion"
				testRuntime "dk.brics.automaton:automaton:1.11-8"
			}
			break;

		// Hadoop 2.0 Alpha
		case "hadoop20":
			dependencies {
				testCompile "org.apache.hadoop:hadoop-mapreduce-examples:$hadoopVersion"
				testRuntime "org.apache.hadoop:hadoop-mapreduce-client-jobclient:$hadoopVersion"
				testRuntime "dk.brics.automaton:automaton:1.11-8"
			}
			break;

		// Hadoop 1.2.x Beta
		case "hadoop12":
			dependencies {
				testCompile "org.apache.hadoop:hadoop-examples:$hadoopVersion"
			}
			break;

		// Hadoop 1.1.x
		case "hadoop11":
			dependencies {
				testCompile "org.apache.hadoop:hadoop-examples:$hadoopVersion"
			}
			break;

		default:
			dependencies {
				testCompile "org.apache.hadoop:hadoop-examples:$hadoopVersion"
			}
	}

	dependencies {
		compile hadoop

		// Logging - using commons-logging from spring-core
		testRuntime("log4j:log4j:$log4jVersion")

		// Spring Framework
		// context-support -> spring-aop/beans/core -> commons-logging
		compile "org.springframework:spring-context-support:$springVersion"
		// used for DAO exceptions by Pig/HBase/Hive packages
		optional("org.springframework:spring-tx:$springVersion")
		// used by Hive package
		optional("org.springframework:spring-jdbc:$springVersion")
	
		// Missing dependency in Hadoop 1.0.3
		testRuntime "commons-io:commons-io:$commonsioVersion"
		testRuntime "org.codehaus.jackson:jackson-mapper-asl:$jacksonVersion"
		testRuntime "cglib:cglib:$cglibVersion"

		// Hive
		optional("$hiveGroup:hive-service:$hiveVersion")
	
		// needed by JDBC test
		testRuntime "$hiveGroup:hive-jdbc:$hiveVersion"
	
		// needed by the Hive Server tests
		// testRuntime "$hiveGroup:hive-builtins:$hiveVersion" 
		// testRuntime("$hiveGroup:hive-metastore:$hiveVersion") 

		//testRuntime "$hiveGroup:hive-common:$hiveVersion"
		//testRuntime "$hiveGroup:hive-shims:$hiveVersion"
		//testRuntime "$hiveGroup:hive-serde:$hiveVersion"
		//testRuntime "org.apache.thrift:libthrift:$thriftVersion"
		//testRuntime "org.apache.thrift:libfb303:$thriftVersion"

		// Pig
		optional("org.apache.pig:pig:$pigVersion$pigQualifier")
	
		// HBase
		optional("org.apache.hbase:hbase:$hbaseVersion") { dep ->
			exclude module: "thrift"
		}

		// Libs dependencies (specified to cope with incompatibilities between them)
		// testRuntime "org.antlr:antlr:$antlrVersion"
		// testRuntime "org.antlr:antlr-runtime:$antlrVersion"

	
		// Testing
		testCompile "junit:junit:$junitVersion"
		testCompile "org.mockito:mockito-core:$mockitoVersion"
		testCompile "org.springframework:spring-test:$springVersion"
		testCompile("javax.annotation:jsr250-api:1.0")
	
		testRuntime "org.codehaus.groovy:groovy:$groovyVersion"
		testRuntime "org.jruby:jruby:$jrubyVersion"
		testRuntime "org.python:jython-standalone:$jythonVersion"
	
		// specify a version of antlr that works with both hive and pig (works only during compilation)
		testRuntime "org.antlr:antlr-runtime:$antlrVersion"
	}

}

project('spring-data-hadoop-core') {
	description = 'Spring for Apache Hadoop Core'
}

project('spring-data-hadoop-batch') {
	description = 'Spring for Apache Hadoop Batch Features'

	dependencies {
		compile project(":spring-data-hadoop-core")
		compile "org.springframework.batch:spring-batch-core:$springBatchVersion"
		testRuntime "org.springframework.integration:spring-integration-file:$springIntVersion"
	}

}

project('spring-data-hadoop') {
	description = 'Spring for Apache Hadoop Configuration'

	dependencies {
		compile project(":spring-data-hadoop-core")
		compile project(":spring-data-hadoop-batch")
		compile project(":spring-cascading")
	}

}

project('spring-hadoop-test') {
	description = 'Spring for Apache Hadoop Integration Tests'

	dependencies {
		compile project(":spring-data-hadoop-core")
		compile project(":spring-data-hadoop-batch")
		compile project(":spring-data-hadoop")
		compile project(":spring-cascading")

		// Testing
		testCompile "junit:junit:$junitVersion"
		testCompile "org.mockito:mockito-core:$mockitoVersion"
		testCompile "org.springframework:spring-test:$springVersion"
		testCompile("javax.annotation:jsr250-api:1.0")
		testCompile "org.springframework.integration:spring-integration-stream:$springIntVersion"
		testCompile "org.springframework.integration:spring-integration-file:$springIntVersion"
		testRuntime "org.springframework.integration:spring-integration-event:$springIntVersion"

		testRuntime "cglib:cglib:$cglibVersion"
		testRuntime "commons-io:commons-io:$commonsioVersion"

		testCompile "cascading:cascading-local:$cascadingVersion"

	}

	task downloadGutenbergBooks {
		ant.get(src: 'http://www.gutenberg.lib.md.us/1/0/100/100.txt', 
				dest: 'src/test/resources/input/gutenberg',skipexisting:true)
		ant.get(src: 'http://www.gutenberg.lib.md.us/1/3/135/135.txt', 
				dest: 'src/test/resources/input/gutenberg',skipexisting:true)
		ant.get(src: 'http://www.gutenberg.lib.md.us/1/3/9/1399/1399.txt', 
				dest: 'src/test/resources/input/gutenberg',skipexisting:true)
		ant.get(src: 'http://www.gutenberg.lib.md.us/2/6/0/2600/2600.txt', 
				dest: 'src/test/resources/input/gutenberg',skipexisting:true)
	}

	task enablePigTests {
		description = "Enabling Pig tests"
		group = "Verification"
		doLast() {
			project.ext.skipPig = false
		}
	}

	task enableHiveTests {
		description = "Enabling Hive tests"
		group = "Verification"
		doLast() {
			project.ext.skipHive = false
		}
	}

	task enableHBaseTests {
		description = "Enabling HBase tests"
		group = "Verification"
		doLast() {
			project.ext.skipHBase = false
		}
	}

	task enableWebHdfsTests {
		description = "Enabling WebHdfs tests"
		group = "Verification"
		doLast() {
			project.ext.skipWebHdfs = false
		}
	}

	task enableCascadingTests {
		description = "Enabling Cascading tests"
		group = "Verification"
		doLast() {
			project.ext.skipCascading = false
		}
	}

	task enableAllTests() {
		description = "Enabling all (incl. Pig, Hive, HBase, WebHdfs, Cascading) tests"
		group = "Verification"
		doLast() {
			project.ext.skipPig = false
			project.ext.skipHive = false
			project.ext.skipHBase = false
			project.ext.skipWebHdfs = false
			project.ext.skipCascading = false
		}
	}

	test {
		//forkEvery = 1
		systemProperties['input.path'] = 'build/classes/test/input'
		systemProperties['output.path'] = 'build/classes/test/output'
		includes = ["**/*.class"]

		testLogging {
			events "started"
			minGranularity 2
			maxGranularity 2
		}

		doFirst() {
			ext.msg = " "

			if (project.ext.skipPig) {
				ext.msg += "Pig "
				excludes.add("**/pig/**")
			}
			if (project.ext.skipHBase) {
				ext.msg += "HBase "
				excludes.add("**/hbase/**")
			}
	
			if (project.ext.skipHive) {
				ext.msg += "Hive "
				excludes.add("**/hive/**")
			}

			if (project.ext.skipWebHdfs) {
				ext.msg += "WebHdfs "
				excludes.add("**/WebHdfs*")
			}

			if (project.ext.skipCascading) {
				ext.msg += "Cascading "
				excludes.add("**/cascading/**")
			}
	
			if (!msg.trim().isEmpty())
					println "Skipping [$msg] Tests";
	
			// check prefix for hd.fs
			// first copy the properties since we can't change them
			ext.projProps = project.properties
	
			if (projProps.containsKey("hd.fs")) {
					String hdfs = projProps["hd.fs"].toString()
					if (!hdfs.contains("://")) {
						projProps.put("hd.fs", "hdfs://" + hdfs)
					}
			}
			
			// due to GRADLE-2475, set the system properties manually
			projProps.each { k,v ->
					if (k.toString().startsWith("hd.")) {
						systemProperties[k] = projProps[k]
					}
			}
		}
	}

}

project('spring-cascading') {
	description = 'Spring Cascading Support'

	dependencies {
		compile project(":spring-data-hadoop-batch")

		compile "org.springframework.integration:spring-integration-core:$springIntVersion"
		// cascading
		compile("cascading:cascading-hadoop:$cascadingVersion") { dep ->
			exclude module: "hadoop-core"
		}

	}

}

if (hadoopVersion >= "2") {
	project('spring-yarn') {
		description = 'Spring for Apache Hadoop YARN'
	}
}

task wrapper(type: Wrapper) {
	description = "Generates gradlew[.bat] scripts"
	gradleVersion = "1.6"
}
	
