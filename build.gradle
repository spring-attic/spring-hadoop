// used for artifact names, building doc upload urls, etc.
description = 'Spring Data Hadoop'
abbreviation = 'SHDP'

apply plugin: 'base'

buildscript {
    repositories {
        add(new org.apache.ivy.plugins.resolver.URLResolver()) {
            name = "GitHub"
            addIvyPattern 'http://cloud.github.com/downloads/costin/gradle-stuff/[organization].[module]-[artifact]-[revision].[ext]'
            addArtifactPattern 'http://cloud.github.com/downloads/costin/gradle-stuff/[organization].[module]-[revision].[ext]'
        }
        mavenCentral()
        mavenLocal()
        mavenRepo name: "springsource-org-release", urls: "http://repository.springsource.com/maven/bundles/release"
        mavenRepo name: "springsource-org-external", urls: "http://repository.springsource.com/maven/bundles/external"
        
    }

    dependencies {
        classpath 'org.springframework:gradle-stuff:0.1-20110421'
        classpath 'net.sf.docbook:docbook-xsl:1.75.2:ns-resources@zip'
    }
}

allprojects {
    group = 'org.springframework.data.hadoop'
    version = "$springHadoopVersion"
    
    releaseBuild = version.endsWith('RELEASE')
    snapshotBuild = version.endsWith('SNAPSHOT')


    repositories {
        mavenLocal()
        mavenCentral()
        // Public Spring artefacts
        mavenRepo name: "springsource-org-release", urls: "http://repository.springsource.com/maven/bundles/release"
        mavenRepo name: "spring-release", urls: "http://maven.springframework.org/release"
        mavenRepo name: "spring-milestone", urls: "http://maven.springframework.org/milestone"
        mavenRepo name: "spring-snapshot", urls: "http://maven.springframework.org/snapshot"
        mavenRepo name: "sonatype-snapshot", urls: "http://oss.sonatype.org/content/repositories/snapshots"
        mavenRepo name: "ext-snapshots", urls: "http://springframework.svn.sourceforge.net/svnroot/springframework/repos/repo-ext/"
        mavenRepo name: "data-nucleus", urls: "http://www.datanucleus.org/downloads/maven2/"
        mavenRepo name: "conjars.org", urls: "http://conjars.org/repo"
	}
}

apply plugin: "java"
apply plugin: "maven"
apply plugin: 'eclipse'  // `gradle eclipse` to generate .classpath/.project
apply plugin: 'idea'     // `gradle idea` to generate .ipr/.iml
apply plugin: 'docbook'

skipPig = true
skipHive = true
skipHBase = true

task enablePigTests {
    description = "Enable Pig tests"
    group = "Verification"
    
    doLast() {
        project.skipPig = false
   }
}

task enableHiveTests {
    description = "Enable Hive tests"
    group = "Verification"
    doLast() {
        project.skipHive = false
   }
}

task enableHBaseTests {
    description = "Enable HBase tests"
    group = "Verification"
    doLast() {
        project.skipHBase = false
    }
}

task enableAllTests() {
    description = "Enable all (incl. Pig, Hive, HBase) tests"
    group = "Verification"
    doFirst() {
      println "Enable all tests"
      project.skipPig = false
      project.skipHBase = false
      project.skipHive = false
    }
}

test {
    //forkEvery = 1
    systemProperties['input.path'] = 'build/classes/test/input'
    systemProperties['output.path'] = 'build/classes/test/output'
    includes = ["**/*.class"]

    doFirst() {
        msg = ""
        
        if (skipPig) 
        {
            msg += "Pig "
            excludes.add("**/pig/**")
        }
        if (skipHBase)
        {
            msg += "HBase "
            excludes.add("**/hbase/**")
        }
        
        if (skipHive) 
        {
            msg += "Hive"
            excludes.add("**/hive/**")
        }
        
        if (!msg.isEmpty())
            println "Skipping [$msg] Tests";
    }
}

// Common dependencies
dependencies {
    // Hadoop
    compile("org.apache.hadoop:hadoop-core:$hadoopVersion") { optional = true }
    compile("org.apache.hadoop:hadoop-streaming:$hadoopVersion") { optional = true }
    compile("org.apache.hadoop:hadoop-tools:$hadoopVersion") { optional = true }

    // Logging
    compile "org.slf4j:slf4j-api:$slf4jVersion"
    compile "org.slf4j:jcl-over-slf4j:$slf4jVersion"
    runtime "log4j:log4j:$log4jVersion"
    runtime "org.slf4j:slf4j-log4j12:$slf4jVersion"
    
    // Spring Framework
    compile("org.springframework:spring-core:$springVersion") {
        exclude module: "commons-logging"
    }
    compile "org.springframework:spring-context-support:$springVersion"
	compile "opensymphony:quartz:$quartzVersion"
    compile("org.springframework:spring-tx:$springVersion") { optional = true }
    compile("org.springframework:spring-aop:$springVersion") { optional = true }
    //compile "org.springframework:spring-oxm:$springVersion"
    compile("org.springframework:spring-jdbc:$springVersion") { optional = true }
    compile("org.springframework.batch:spring-batch-core:$springBatchVersion") { optional = true }
    compile("org.springframework.integration:spring-integration-core:$springIntVersion") { optional = true }

    // cascading
    compile("cascading:cascading-hadoop:$cascadingVersion") { optional = true }
    
    //testRuntime "commons-io:commons-io:$commonsioVersion"
    //testRuntime "org.codehaus.jackson:jackson-core-asl:$jacksonVersion"
    testRuntime "org.codehaus.jackson:jackson-mapper-asl:$jacksonVersion"
    testRuntime "cglib:cglib:$cglibVersion"

    // Hive
    testRuntime "org.apache.hive:hive-common:$hiveVersion"
    testRuntime "org.apache.hive:hive-jdbc:$hiveVersion"
    compile("org.apache.hive:hive-service:$hiveVersion") { optional = true }
    testRuntime "org.apache.hive:hive-metastore:$hiveVersion"
    testRuntime "org.apache.hive:hive-shims:$hiveVersion"
    testRuntime "org.apache.hive:hive-serde:$hiveVersion"
    testRuntime "org.apache.thrift:libthrift:$thriftVersion"
    testRuntime "org.apache.thrift:libfb303:$thriftVersion"

    // Pig
    compile("org.apache.pig:pig:$pigVersion") { optional = true }
    
    // HBase
    compile("org.apache.hbase:hbase:$hbaseVersion") {
        exclude module: "thrift"
        optional = true
    }

    // Libs dependencies (specified to cope with incompatibilities between them)
    testRuntime "org.antlr:antlr:$antlrVersion"
    testRuntime "org.antlr:antlr-runtime:$antlrVersion"

    
    // Testing
    testCompile "junit:junit:$junitVersion"
    testCompile "org.mockito:mockito-core:$mockitoVersion"
    testCompile "org.springframework:spring-test:$springVersion"
    testCompile("javax.annotation:jsr250-api:1.0") { optional = true }
    testCompile "org.apache.hadoop:hadoop-examples:$hadoopVersion"
    testRuntime "org.springframework.integration:spring-integration-event:$springIntVersion"
    testRuntime "org.springframework.integration:spring-integration-file:$springIntVersion"
    testRuntime "org.codehaus.groovy:groovy:$groovyVersion"
    testRuntime "org.jruby:jruby:$jrubyVersion"
    testRuntime "org.python:jython-standalone:$jythonVersion"
    testRuntime "org.apache.hive:hive-builtins:$hiveVersion"
    // specify a version of antlr that works with both hive and pig
    testRuntime "org.antlr:antlr-runtime:$antlrVersion"
    // hive optional dependencies
    //testCompile "org.datanucleus:datanucleus-connectionpool:$dataNucleusVersion"
    //testCompile "org.datanucleus:datanucleus-core:$dataNucleusVersion"
    //testCompile "org.datanucleus:datanucleus-enhancer:$dataNucleusVersion"
    //testCompile "org.datanucleus:datanucleus-rdbms:$dataNucleusVersion"
    //testCompile "javax.jdo:jdo2-api:$jdoVersion"
    
    testCompile "cascading:cascading-local:$cascadingVersion"
}

javaprojects = rootProject

sourceCompatibility = 1.6
targetCompatibility = 1.6 

javadoc {
  srcDir = file("${projectDir}/docs/src/api")
  destinationDir = file("${buildDir}/api")
  tmpDir = file("${buildDir}/api-work")
  
  configure(options) {
      stylesheetFile = file("${srcDir}/spring-javadoc.css")
      overview = "${srcDir}/overview.html"
      docFilesSubDirs = true
      outputLevel = org.gradle.external.javadoc.JavadocOutputLevel.QUIET
      breakIterator = true
      author = true
      showFromProtected()
      
//      groups = [
//        'Spring Data Hadoop' : ['org.springframework.data.hadoop*'],
//      ]
  
     links = [
        "http://static.springframework.org/spring/docs/3.0.x/javadoc-api",
        "http://download.oracle.com/javase/6/docs/api",
        "http://logging.apache.org/log4j/docs/api/",
        "http://hadoop.apache.org/common/docs/current/api/",
        "http://hbase.apache.org/apidocs/",
        "http://pig.apache.org/docs/r0.9.2/api/",
        "http://hive.apache.org/docs/r0.8.1/api/",
        "http://static.springsource.org/spring-batch/apidocs/",
        "http://static.springsource.org/spring-integration/api/",
        "https://builds.apache.org/job/Thrift/javadoc/",
        "http://jakarta.apache.org/commons/logging/apidocs/",
        "http://www.cascading.org/1.2/javadoc/"
     ]
     
     exclude "org/springframework/data/hadoop/config/**"
  }
    
  title = "${rootProject.description} ${version} API"

  // collect all the sources that will be included in the javadoc output
  source javaprojects.collect {project ->
      project.sourceSets.main.allJava
  }

  // collect all main classpaths to be able to resolve @see refs, etc.
  // this collection also determines the set of projects that this
  // task dependsOn, thus the runtimeClasspath is used to ensure all
  // projects are included, not just *dependencies* of all classes.
  // this is awkward and took me a while to figure out.
  classpath = files(javaprojects.collect {project ->
      project.sourceSets.main.runtimeClasspath
  })

  // copy the images from the doc-files dir over to the target
  doLast { task ->
      copy {
          from file("${task.srcDir}/doc-files")
          into file("${task.destinationDir}/doc-files")
      }
  }
}

ideaProject {
	withXml { provider ->
		provider.node.component.find { it.@name == 'VcsDirectoryMappings' }.mapping.@vcs = 'Git'
	}
}

task wrapper(type: Wrapper) {
    gradleVersion = '1.0-milestone-3'
    description = "Generate the Gradle wrapper"
    group = "Distribution"
}

apply from: "$rootDir/maven.gradle"

// Distribution tasks
task dist(type: Zip) {
    description = "Generate the ZIP Distribution"
    group = "Distribution"
    dependsOn assemble, subprojects*.tasks*.matching { task -> task.name == 'assemble' }

    evaluationDependsOn(':docs')

    def zipRootDir = "${project.name}-$version"
    into(zipRootDir) {
        from('/docs/src/info') {
            include '*.txt'
        }
        from('/docs/build/') {
            into 'docs'
            include 'reference/**/*'
        }
        from('samples/') {
            into 'samples'
            exclude '**/build/**'
            exclude '**/bin/**'
            exclude '**/.settings/**'
            exclude '**/.gradle/**'
            exclude '**/.*'
        }
        from('build/') {
            into 'docs'
            include 'api/**/*'
        }
            into('dist') {
            from javaprojects.collect {project -> project.libsDir }
        }
    }
    doLast {
        ant.checksum(file: archivePath, algorithm: 'SHA1', fileext: '.sha1')
    }
}

task uploadDist(type: org.springframework.gradle.tasks.S3DistroUpload, dependsOn: dist) {
    description = "Upload the ZIP Distribution"
    group = "Distribution"
    archiveFile = dist.archivePath
    projectKey = 'SHDP'
    projectName = 'Spring Data Hadoop'
}

assemble.dependsOn = ['jar', 'sourceJar', 'javadocJar']
defaultTasks 'build'