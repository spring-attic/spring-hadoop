description = 'Spring for Apache Hadoop'

defaultTasks 'build'

buildscript {
	repositories {
		maven { url "http://repo.spring.io/plugins-release" }
	}
	dependencies {
		classpath("org.springframework.boot:spring-boot-gradle-plugin:$springBootVersion")
		classpath("org.springframework.build.gradle:propdeps-plugin:0.0.5")
		classpath("org.springframework.build.gradle:docbook-reference-plugin:0.2.8")
		classpath("org.springframework.build.gradle:spring-io-plugin:0.0.3.RELEASE")
	}
}

allprojects {
	group = 'org.springframework.data'

	repositories {
		mavenCentral()
		maven { url 'http://repo.spring.io/libs-release' }
	}
}

def javaProjects() {
	subprojects.findAll { project -> project.name != 'docs' }
}

def hadoopProjects() {
	subprojects.findAll { project -> project.name.contains('-hadoop-') }
}

def yarnProjects() {
	subprojects.findAll { project -> project.name.contains('-yarn-') }
}

println "Using Spring Framework version: [$springVersion]"
println "Using Java version: [" + System.getProperty("java.version") + "]"

//
//  Select the Hadoop distribution used for building the binaries
//
def List hadoopArtifacts = []
def List hadoopTestArtifacts = []
def hadoopDefault = "hadoop22"
def hadoopDistro = project.hasProperty("distro") ? project.getProperty("distro") : hadoopDefault
def hadoopVersion = hd22Version

// Common Hadoop libraries
def hiveVersion = defaultHiveVersion
def pigVersion = defaultPigVersion
def hbaseVersion = defaultHbaseVersion
def List hbaseArtifacts = []

// handle older Hive version
def hiveGroup = "org.apache.hive"

// make it possible to use Pig jars compiled for Hadoop 2.0
def pigQualifier = ''

// default is Hadoop 2.2.x
switch (hadoopDistro) {

	// Cloudera CDH5 YARN 2.2.x base
	case "cdh5":
		hadoopVersion = cdh5Version
		println "Using Cloudera CDH5 [$hadoopVersion]"
		hbaseVersion = cdh5HbaseVersion
		hbaseArtifacts = ["org.apache.hbase:hbase:$hbaseVersion",
				  "org.apache.hbase:hbase-common:$hbaseVersion",
				  "org.apache.hbase:hbase-client:$hbaseVersion"]
		hiveVersion = cdh5HiveVersion
		pigVersion = cdh5PigVersion
		hadoopArtifacts = ["org.apache.hadoop:hadoop-common:$hadoopVersion",
				   "org.apache.hadoop:hadoop-hdfs:$hadoopVersion",
				   "org.apache.hadoop:hadoop-mapreduce-client-core:$hadoopVersion",
				   "org.apache.hadoop:hadoop-mapreduce-client-jobclient:$hadoopVersion",
				   "org.apache.hadoop:hadoop-yarn-common:$hadoopVersion",
				   "org.apache.hadoop:hadoop-streaming:$hadoopVersion",
				   "org.apache.hadoop:hadoop-distcp:$hadoopVersion"]
		hadoopTestArtifacts = ["org.apache.hadoop:hadoop-mapreduce-client-jobclient:$hadoopVersion:tests",
					"org.apache.hadoop:hadoop-hdfs:$hadoopVersion:tests",
					"org.apache.hadoop:hadoop-common:$hadoopVersion:tests",
					"org.apache.hadoop:hadoop-mapreduce-client-app:$hadoopVersion",
					"org.apache.hadoop:hadoop-mapreduce-client-hs:$hadoopVersion",
					"org.apache.hadoop:hadoop-yarn-server-tests:$hadoopVersion",
					"org.apache.hadoop:hadoop-yarn-server-tests:$hadoopVersion:tests"]
		break;

	// Cloudera CDH4 YARN
	case "cdh4":
		hadoopVersion = cdh4Version
		println "Using Cloudera CDH4 [$hadoopVersion]"
		hbaseVersion = cdh4HbaseVersion
		hbaseArtifacts = ["org.apache.hbase:hbase:$hbaseVersion"]
		hiveVersion = cdh4HiveVersion
		pigVersion = cdh4PigVersion
		hadoopArtifacts = ["org.apache.hadoop:hadoop-common:$hadoopVersion",
				   "org.apache.hadoop:hadoop-hdfs:$hadoopVersion",
				   "org.apache.hadoop:hadoop-mapreduce-client-core:$hadoopVersion",
				   "org.apache.hadoop:hadoop-mapreduce-client-jobclient:$hadoopVersion",
				   "org.apache.hadoop:hadoop-yarn-common:$hadoopVersion",
				   "org.apache.hadoop:hadoop-streaming:$hadoopVersion",
				   "org.apache.hadoop:hadoop-distcp:$hadoopVersion"]
		hadoopTestArtifacts = ["org.apache.hadoop:hadoop-mapreduce-client-jobclient:$hadoopVersion:tests",
					"org.apache.hadoop:hadoop-hdfs:$hadoopVersion:tests",
					"org.apache.hadoop:hadoop-common:$hadoopVersion:tests",
					"org.apache.hadoop:hadoop-mapreduce-client-app:$hadoopVersion",
					"org.apache.hadoop:hadoop-mapreduce-client-hs:$hadoopVersion",
					"org.apache.hadoop:hadoop-yarn-server-tests:$hadoopVersion",
					"org.apache.hadoop:hadoop-yarn-server-tests:$hadoopVersion:tests"]
		break;	

	// Pivotal HD 1.0
	case "phd1":
		hadoopVersion = phd1Version
		println "Using Pivotal HD 1.0 - [$hadoopVersion]"
		hbaseVersion = phd1HbaseVersion
		hbaseArtifacts = ["org.apache.hbase:hbase:$hbaseVersion"]
		hiveVersion = phd1HiveVersion
		pigVersion = phd1PigVersion
		hadoopArtifacts = ["org.apache.hadoop:hadoop-common:$hadoopVersion",
				   "org.apache.hadoop:hadoop-hdfs:$hadoopVersion",
				   "org.apache.hadoop:hadoop-mapreduce-client-core:$hadoopVersion",
				   "org.apache.hadoop:hadoop-mapreduce-client-jobclient:$hadoopVersion",
				   "org.apache.hadoop:hadoop-yarn-common:$hadoopVersion",
				   "org.apache.hadoop:hadoop-streaming:$hadoopVersion",
				   "org.apache.hadoop:hadoop-distcp:$hadoopVersion"]
		hadoopTestArtifacts = ["org.apache.hadoop:hadoop-mapreduce-client-jobclient:$hadoopVersion:tests",
					"org.apache.hadoop:hadoop-hdfs:$hadoopVersion:tests",
					"org.apache.hadoop:hadoop-common:$hadoopVersion:tests",
					"org.apache.hadoop:hadoop-mapreduce-client-app:$hadoopVersion",
					"org.apache.hadoop:hadoop-mapreduce-client-hs:$hadoopVersion",
					"org.apache.hadoop:hadoop-yarn-server-tests:$hadoopVersion",
					"org.apache.hadoop:hadoop-yarn-server-tests:$hadoopVersion:tests"]
		break;

	// Pivotal HD 2.0
	case "phd20":
		hadoopVersion = phd20Version
		println "Using Pivotal HD 2.0 - [$hadoopVersion]"
		hadoopArtifacts = ["org.apache.hadoop:hadoop-common:$hadoopVersion",
				   "org.apache.hadoop:hadoop-hdfs:$hadoopVersion",
				   "org.apache.hadoop:hadoop-mapreduce-client-core:$hadoopVersion",
				   "org.apache.hadoop:hadoop-mapreduce-client-jobclient:$hadoopVersion",
				   "org.apache.hadoop:hadoop-yarn-common:$hadoopVersion",
				   "org.apache.hadoop:hadoop-streaming:$hadoopVersion",
				   "org.apache.hadoop:hadoop-distcp:$hadoopVersion"]
		hadoopTestArtifacts = ["org.apache.hadoop:hadoop-mapreduce-client-jobclient:$hadoopVersion:tests",
					"org.apache.hadoop:hadoop-hdfs:$hadoopVersion:tests",
					"org.apache.hadoop:hadoop-common:$hadoopVersion:tests",
					"org.apache.hadoop:hadoop-mapreduce-client-app:$hadoopVersion",
					"org.apache.hadoop:hadoop-mapreduce-client-hs:$hadoopVersion",
					"org.apache.hadoop:hadoop-yarn-server-tests:$hadoopVersion",
					"org.apache.hadoop:hadoop-yarn-server-tests:$hadoopVersion:tests"]
		hbaseVersion = phd20HbaseVersion
		hbaseArtifacts = ["org.apache.hbase:hbase:$hbaseVersion",
				  "org.apache.hbase:hbase-common:$hbaseVersion",
				  "org.apache.hbase:hbase-client:$hbaseVersion"]
		hiveVersion = phd20HiveVersion
		pigVersion = phd20PigVersion
		pigQualifier = ':h2'
		break;

	// Hortonworks Data Platform 2.0
	case "hdp20":
		hadoopVersion = hdp20Version
		println "Using Hortonworks Data Platform 2.0 [$hadoopVersion]"
		hadoopArtifacts = ["org.apache.hadoop:hadoop-common:$hadoopVersion",
				   "org.apache.hadoop:hadoop-hdfs:$hadoopVersion",
				   "org.apache.hadoop:hadoop-mapreduce-client-core:$hadoopVersion",
				   "org.apache.hadoop:hadoop-mapreduce-client-jobclient:$hadoopVersion",
				   "org.apache.hadoop:hadoop-yarn-common:$hadoopVersion",
				   "org.apache.hadoop:hadoop-streaming:$hadoopVersion",
				   "org.apache.hadoop:hadoop-distcp:$hadoopVersion"]
		hadoopTestArtifacts = ["org.apache.hadoop:hadoop-mapreduce-client-jobclient:$hadoopVersion:tests",
					"org.apache.hadoop:hadoop-hdfs:$hadoopVersion:tests",
					"org.apache.hadoop:hadoop-common:$hadoopVersion:tests",
					"org.apache.hadoop:hadoop-mapreduce-client-app:$hadoopVersion",
					"org.apache.hadoop:hadoop-mapreduce-client-hs:$hadoopVersion",
					"org.apache.hadoop:hadoop-yarn-server-tests:$hadoopVersion",
					"org.apache.hadoop:hadoop-yarn-server-tests:$hadoopVersion:tests"]
		hbaseVersion = hdp20HbaseVersion
		hbaseArtifacts = ["org.apache.hbase:hbase:$hbaseVersion",
				  "org.apache.hbase:hbase-common:$hbaseVersion",
				  "org.apache.hbase:hbase-client:$hbaseVersion"]
		hiveVersion = hdp20HiveVersion
		pigVersion = hdp20PigVersion
		pigQualifier = ':h2'
		break;

	// Hortonworks Data Platform 2.1
	case "hdp21":
		hadoopVersion = hdp21Version
		println "Using Hortonworks Data Platform 2.1 [$hadoopVersion]"
		hadoopArtifacts = ["org.apache.hadoop:hadoop-common:$hadoopVersion",
				   "org.apache.hadoop:hadoop-hdfs:$hadoopVersion",
				   "org.apache.hadoop:hadoop-mapreduce-client-core:$hadoopVersion",
				   "org.apache.hadoop:hadoop-mapreduce-client-jobclient:$hadoopVersion",
				   "org.apache.hadoop:hadoop-yarn-common:$hadoopVersion",
				   "org.apache.hadoop:hadoop-streaming:$hadoopVersion",
				   "org.apache.hadoop:hadoop-distcp:$hadoopVersion"]
		hadoopTestArtifacts = ["org.apache.hadoop:hadoop-mapreduce-client-jobclient:$hadoopVersion:tests",
					"org.apache.hadoop:hadoop-hdfs:$hadoopVersion:tests",
					"org.apache.hadoop:hadoop-common:$hadoopVersion:tests",
					"org.apache.hadoop:hadoop-mapreduce-client-app:$hadoopVersion",
					"org.apache.hadoop:hadoop-mapreduce-client-hs:$hadoopVersion",
					"org.apache.hadoop:hadoop-yarn-server-tests:$hadoopVersion",
					"org.apache.hadoop:hadoop-yarn-server-tests:$hadoopVersion:tests"]
		hbaseVersion = hdp21HbaseVersion
		hbaseArtifacts = ["org.apache.hbase:hbase:$hbaseVersion",
				  "org.apache.hbase:hbase-common:$hbaseVersion",
				  "org.apache.hbase:hbase-client:$hbaseVersion"]
		hiveVersion = hdp21HiveVersion
		pigVersion = hdp21PigVersion
		pigQualifier = ':h2'
		break;

	// Hadoop 2.4.x
	case "hadoop24":
		hadoopVersion = hd24Version
		println "Using Apache Hadoop 2.4 - [$hadoopVersion]"
		hadoopArtifacts = ["org.apache.hadoop:hadoop-common:$hadoopVersion",
				   "org.apache.hadoop:hadoop-hdfs:$hadoopVersion",
				   "org.apache.hadoop:hadoop-mapreduce-client-core:$hadoopVersion",
				   "org.apache.hadoop:hadoop-mapreduce-client-jobclient:$hadoopVersion",
				   "org.apache.hadoop:hadoop-yarn-common:$hadoopVersion",
				   "org.apache.hadoop:hadoop-streaming:$hadoopVersion",
				   "org.apache.hadoop:hadoop-distcp:$hadoopVersion"]
		hadoopTestArtifacts = ["org.apache.hadoop:hadoop-mapreduce-client-jobclient:$hadoopVersion:tests",
					"org.apache.hadoop:hadoop-hdfs:$hadoopVersion:tests",
					"org.apache.hadoop:hadoop-common:$hadoopVersion:tests",
					"org.apache.hadoop:hadoop-mapreduce-client-app:$hadoopVersion",
					"org.apache.hadoop:hadoop-mapreduce-client-hs:$hadoopVersion",
					"org.apache.hadoop:hadoop-yarn-server-tests:$hadoopVersion",
					"org.apache.hadoop:hadoop-yarn-server-tests:$hadoopVersion:tests"]
		hbaseVersion = hd24HbaseVersion
		hbaseArtifacts = ["org.apache.hbase:hbase:$hbaseVersion",
				  "org.apache.hbase:hbase-common:$hbaseVersion",
				  "org.apache.hbase:hbase-client:$hbaseVersion"]
		hiveVersion = hd24HiveVersion
		pigVersion = hd24PigVersion
		pigQualifier = ':h2'
		break;

	// Hadoop 2.2.x Stable
	default:
		if (!project.hasProperty("distro")) {
			println "Using default distro: Apache Hadoop [$hadoopVersion]"
		} else {
			if (hadoopDistro == hadoopDefault) {
				println "Using Apache Hadoop 2.2 - [$hadoopVersion]"
			} else {
				println "Failing build: $hadoopDistro is not a supported distro"
				println "Supported distros: hadoop22[*], hadoop24, hdp20, cdh4, cdh5, phd1 and phd20"
				println "* default"
				throw new InvalidUserDataException("$hadoopDistro is not a supported distro")
			}
		}
		hadoopVersion = hd22Version
		hadoopArtifacts = ["org.apache.hadoop:hadoop-common:$hadoopVersion",
				   "org.apache.hadoop:hadoop-hdfs:$hadoopVersion",
				   "org.apache.hadoop:hadoop-mapreduce-client-core:$hadoopVersion",
				   "org.apache.hadoop:hadoop-mapreduce-client-jobclient:$hadoopVersion",
				   "org.apache.hadoop:hadoop-yarn-common:$hadoopVersion",
				   "org.apache.hadoop:hadoop-streaming:$hadoopVersion",
				   "org.apache.hadoop:hadoop-distcp:$hadoopVersion"]
		hadoopTestArtifacts = ["org.apache.hadoop:hadoop-mapreduce-client-jobclient:$hadoopVersion:tests",
					"org.apache.hadoop:hadoop-hdfs:$hadoopVersion:tests",
					"org.apache.hadoop:hadoop-common:$hadoopVersion:tests",
					"org.apache.hadoop:hadoop-mapreduce-client-app:$hadoopVersion",
					"org.apache.hadoop:hadoop-mapreduce-client-hs:$hadoopVersion",
					"org.apache.hadoop:hadoop-yarn-server-tests:$hadoopVersion",
					"org.apache.hadoop:hadoop-yarn-server-tests:$hadoopVersion:tests"]
		hbaseVersion = hd22HbaseVersion
		hbaseArtifacts = ["org.apache.hbase:hbase:$hbaseVersion",
				  "org.apache.hbase:hbase-common:$hbaseVersion",
				  "org.apache.hbase:hbase-client:$hbaseVersion"]
		hiveVersion = hd22HiveVersion
		pigVersion = hd22PigVersion
		pigQualifier = ':h2'
}

configure(javaProjects()) {

	apply plugin: 'java'
	apply from:   "${rootProject.projectDir}/maven.gradle"
	apply plugin: 'eclipse'
	apply plugin: 'idea'
	apply plugin: 'propdeps'
	apply plugin: 'propdeps-idea'
	apply plugin: 'propdeps-eclipse'

	if (project.hasProperty('platformVersion')) {
		apply plugin: 'spring-io'

		repositories {
			maven { url "https://repo.spring.io/libs-snapshot" }
		}

		dependencies {
			springIoVersions "io.spring.platform:platform-versions:${platformVersion}@properties"
		}
	}

	sourceCompatibility=1.6
	targetCompatibility=1.6

	// assume we are skipping these tests (must be enabled explicitly)
	ext.skipPig = true
	ext.skipHive = true
	ext.skipHBase = true
	ext.skipWebHdfs = true

	// exclude poms from the classpath (pulled in by Cloudera)
	eclipse.classpath.file {
		whenMerged { classpath ->
			classpath.entries.removeAll { entry -> entry.toString().contains(".pom") }
		}
	}

	eclipse {
		project {
			natures += 'org.springframework.ide.eclipse.core.springnature'
		}
	}

	// dependencies that are common across all java projects
	dependencies {
		compile "org.springframework:spring-aop:$springVersion"
		compile "org.springframework:spring-context:$springVersion"
		compile "org.springframework:spring-context-support:$springVersion"
		compile "org.springframework:spring-jdbc:$springVersion"
		compile "org.springframework:spring-tx:$springVersion"
	}

	task sourcesJar(type: Jar) {
		classifier = 'sources'
		from sourceSets.main.allJava
	}

	task testJar(type: Jar) {
		classifier = 'tests'
		from sourceSets.test.output
	}

	task javadocJar(type: Jar) {
		classifier = 'javadoc'
		from javadoc
	}

	artifacts {
		archives sourcesJar
		archives javadocJar
	}

	assemble.dependsOn = ['jar', 'sourcesJar', 'testJar']

	javadoc {
		ext.srcDir = file("${projectDir}/docs/src/api")
	  
		configure(options) {
			stylesheetFile = file("${rootProject.projectDir}/docs/src/api/stylesheet.css")
			overview = "${rootProject.projectDir}/docs/src/api/overview.html"
			docFilesSubDirs = true
			outputLevel = org.gradle.external.javadoc.JavadocOutputLevel.QUIET
			breakIterator = true
			author = true
			showFromProtected()

	//		groups = [
	//		'Spring Data Hadoop' : ['org.springframework.data.hadoop*'],
	//		]
  
			links = [
				"http://docs.spring.io/spring/docs/4.0.x/javadoc-api/",
				"http://docs.oracle.com/javase/6/docs/api/",
				"http://commons.apache.org/proper/commons-logging/apidocs/",
				"http://logging.apache.org/log4j/1.2/apidocs/",
				"http://hadoop.apache.org/common/docs/current/api/",
				"http://hbase.apache.org/apidocs/",
				"http://pig.apache.org/docs/r0.12.0/api/",
				"http://hive.apache.org/javadocs/r0.12.0/api/",
				"http://docs.spring.io/spring-batch/apidocs/",
				"http://docs.spring.io/spring-integration/api/"
			]

			exclude "org/springframework/data/hadoop/config/**"
	  	}

		title = "${rootProject.description} ${version} API"
	}

	jar {
		manifest.attributes["Created-By"] = "${System.getProperty("java.version")} (${System.getProperty("java.specification.vendor")})"
		manifest.attributes['Implementation-Title'] = 'spring-data-hadoop'
		manifest.attributes['Implementation-Version'] = project.version
		manifest.attributes['Implementation-URL'] = "http://projects.spring.io/spring-hadoop/"
		manifest.attributes['Implementation-Vendor'] = "Spring by Pivotal"
		manifest.attributes['Implementation-Vendor-Id'] = "org.springframework"

		def build = System.env['SHDP.BUILD']
		if (build != null)
			manifest.attributes['Build'] = build

		String rev = "unknown"

		// parse the git files to find out the revision
		File gitHead = file('.git/HEAD')
		if (gitHead.exists()) {
			gitHead = file('.git/' + gitHead.text.trim().replace('ref: ',''))
			if (gitHead.exists()) { rev = gitHead.text }
		}

		from("$rootDir/docs/src/info") {
			include "license.txt"
			include "notice.txt"
			into "META-INF"
			expand(copyright: new Date().format('yyyy'), version: project.version)
		}

		manifest.attributes['Repository-Revision'] = rev
	}

}

configure(hadoopProjects()) {

	// default is Hadoop 2.2.x
	switch (hadoopDistro) {

		// Cloudera CDH5 YARN
		case "cdh5":
		case "cdh5yarn":
			dependencies {
				compile("org.apache.hadoop:hadoop-common:$cdh5Version")
				compile("org.apache.hadoop:hadoop-mapreduce-client-core:$cdh5Version")
				compile("org.apache.hadoop:hadoop-distcp:$cdh5Version")
				compile("org.apache.hadoop:hadoop-hdfs:$cdh5Version")
				optional("org.apache.hadoop:hadoop-streaming:$cdh5Version")
				testCompile "org.apache.hadoop:hadoop-mapreduce-examples:$hadoopVersion"
				testRuntime "org.apache.hadoop:hadoop-mapreduce-client-jobclient:$hadoopVersion"
				testRuntime "dk.brics.automaton:automaton:1.11-8"
			}
			break;	

		// Cloudera CDH4 YARN
		case "cdh4yarn":
			dependencies {
				compile("org.apache.hadoop:hadoop-common:$cdh4Version")
				compile("org.apache.hadoop:hadoop-mapreduce-client-core:$cdh4Version")
				compile("org.apache.hadoop:hadoop-distcp:$cdh4Version")
				compile("org.apache.hadoop:hadoop-hdfs:$cdh4Version")
				optional("org.apache.hadoop:hadoop-streaming:$cdh4Version")
				testCompile "org.apache.hadoop:hadoop-mapreduce-examples:$hadoopVersion"
				testRuntime "org.apache.hadoop:hadoop-mapreduce-client-jobclient:$hadoopVersion"
				testRuntime "dk.brics.automaton:automaton:1.11-8"
			}
			break;	

		// Pivotal HD 1.0
		case "phd1":
			dependencies {
				compile "org.apache.hive:hive-common:$hiveVersion"
				compile "org.apache.hive:hive-metastore:$hiveVersion"
				compile "org.apache.hive:hive-exec:$hiveVersion"
				testCompile "org.apache.hadoop:hadoop-mapreduce-examples:$hadoopVersion"
				testRuntime "org.apache.hadoop:hadoop-mapreduce-client-jobclient:$hadoopVersion"
				testRuntime "dk.brics.automaton:automaton:1.11-8"
			}
			break;

		// Pivotal HD 2.0
		case "phd20":
			dependencies {
				testCompile "org.apache.hadoop:hadoop-mapreduce-examples:$hadoopVersion"
				testRuntime "org.apache.hadoop:hadoop-mapreduce-client-jobclient:$hadoopVersion"
				testRuntime "dk.brics.automaton:automaton:1.11-8"
			}
			break;

		// Hortonworks Data Platform 2.0
		case "hdp20":
			dependencies {
				testCompile "org.apache.hadoop:hadoop-mapreduce-examples:$hadoopVersion"
				testRuntime "org.apache.hadoop:hadoop-mapreduce-client-jobclient:$hadoopVersion"
				testRuntime "dk.brics.automaton:automaton:1.11-8"
			}
			break;

		// Hortonworks Data Platform 2.1
		case "hdp21":
			dependencies {
				testCompile "org.apache.hadoop:hadoop-mapreduce-examples:$hadoopVersion"
				testRuntime "org.apache.hadoop:hadoop-mapreduce-client-jobclient:$hadoopVersion"
				testRuntime "dk.brics.automaton:automaton:1.11-8"
			}
			break;

		// Hadoop 2.4.x
		case "hadoop24":
			dependencies {
				testCompile "org.apache.hadoop:hadoop-mapreduce-examples:$hadoopVersion"
				testRuntime "org.apache.hadoop:hadoop-mapreduce-client-jobclient:$hadoopVersion"
				testRuntime "dk.brics.automaton:automaton:1.11-8"
			}
			break;

		// Hadoop 2.2.x Stable
		default:
			dependencies {
				testCompile "org.apache.hadoop:hadoop-mapreduce-examples:$hadoopVersion"
				testRuntime "org.apache.hadoop:hadoop-mapreduce-client-jobclient:$hadoopVersion"
				testRuntime "dk.brics.automaton:automaton:1.11-8"
			}
	}

	dependencies {
		hadoopArtifacts.each {
			compile(it) { dep ->
				if (it.contains("hadoop-common") || 
					it.contains("hadoop-yarn-common") ||
					it.contains("hadoop-mapreduce-client-jobclient") || 
					it.contains("hadoop-mapreduce-client-core")) {
					exclude group: "org.slf4j", module: "slf4j-log4j12"
				}
				if (it.contains("hadoop-hdfs") ||
					it.contains("hadoop-common") ||
					it.contains("hadoop-yarn-common") ||
					it.contains("hadoop-mapreduce-client-core")) {
					exclude group: "log4j", module: "log4j"
				}
			}
		}

		// Logging - using commons-logging from spring-core
		testRuntime("log4j:log4j:$log4jVersion")

		// Spring Framework
		// context-support -> spring-aop/beans/core -> commons-logging
		compile "org.springframework:spring-context-support:$springVersion"
		// used for DAO exceptions by Pig/HBase/Hive packages
		optional("org.springframework:spring-tx:$springVersion")
		// used by Hive package
		optional("org.springframework:spring-jdbc:$springVersion")
	
		// Missing dependency in Hadoop 1.0.3
		testRuntime "commons-io:commons-io:$commonsioVersion"
		testRuntime "org.codehaus.jackson:jackson-mapper-asl:$jacksonVersion"
		testRuntime "cglib:cglib-nodep:$cglibVersion"

		// Hive
		optional("$hiveGroup:hive-service:$hiveVersion")
	
		// needed by JDBC test
		testRuntime "$hiveGroup:hive-jdbc:$hiveVersion"
	
		// Pig
		optional("org.apache.pig:pig:$pigVersion$pigQualifier") { dep ->
			exclude group: "junit", module: "junit"
		}
	
		// HBase
		hbaseArtifacts.each {
			optional(it)
		}

		// Testing
		testCompile "junit:junit:$junitVersion"
	}

}

configure(rootProject) {

	apply plugin: 'eclipse'
	apply plugin: 'idea'

	apply plugin: "docbook-reference"

	reference {
		sourceDir = file("docs/src/reference/docbook")
		pdfFilename = "spring-data-hadoop-reference.pdf"
	}

	// don't publish the default jar for the root project
	configurations.archives.artifacts.clear()

	task api(type: Javadoc) {
		group = "Documentation"
		description = "Generates aggregated Javadoc API documentation."
		title = "${rootProject.description} ${version} API"

		dependsOn {
			subprojects.collect {
				it.tasks.getByName("jar")
			}
		}
		options.memberLevel = org.gradle.external.javadoc.JavadocMemberLevel.PROTECTED
		options.author = true
		options.header = rootProject.description
		options.overview = "docs/src/api/overview.html"
		options.stylesheetFile = file("docs/src/api/stylesheet.css")
		options.splitIndex = true
		//options.links(project.ext.javadocLinks)

		source subprojects.collect { project ->
			project.sourceSets.main.allJava
		}

		maxMemory = "1024m"
		destinationDir = new File(buildDir, "api")

		doFirst {
			classpath = files(subprojects.collect { it.sourceSets.main.compileClasspath })
		}
	}

	task docsZip(type: Zip) {
		group = "Distribution"
		baseName = "spring-data-hadoop"
		classifier = "docs"
		description = "Builds -${classifier} archive containing api and reference " +
			"for deployment at http://static.springframework.org/spring-hadoop/docs."

		from("docs/src/info") {
			include "changelog.txt"
		}

		from (api) {
			into "api"
		}

		from (reference) {
			into "reference"
		}
	}

	task schemaZip(type: Zip) {
		group = "Distribution"
		baseName = "spring-data-hadoop"
		classifier = "schema"
		description = "Builds -${classifier} archive containing all " +
			"XSDs for deployment at http://springframework.org/schema."

		subprojects.each { subproject ->
			def Properties schemas = new Properties();

			subproject.sourceSets.main.resources.find {
				it.path.endsWith("META-INF/spring.schemas")
			}?.withInputStream { schemas.load(it) }

			for (def key : schemas.keySet()) {
				def shortName = key.replaceAll(/http.*schema.(.*).spring-.*/, '$1')
				assert shortName != key
				File xsdFile = subproject.sourceSets.main.resources.find {
					it.path.endsWith(schemas.get(key))
				}
				assert xsdFile != null
				into (shortName) {
					from xsdFile.path
				}
			}
		}
	}

	task distZip(type: Zip, dependsOn: [docsZip, schemaZip]) {
		group = "Distribution"
		baseName = "spring-data-hadoop"
		classifier = "dist"
		description = "Builds -${classifier} archive, containing all jars and docs, " +
					"suitable for community download page."

		ext.baseDir = "${baseName}-${project.version}";

		from("docs/src/info") {
			include "readme.txt"
			include "license.txt"
			include "notice.txt"
			into "${baseDir}"
			expand(copyright: new Date().format("yyyy"), version: project.version)
		}

		from(zipTree(docsZip.archivePath)) {
			into "${baseDir}/docs"
		}

		from(zipTree(schemaZip.archivePath)) {
			into "${baseDir}/schema"
		}

		subprojects.each { subproject ->
			into ("${baseDir}/libs") {
				from subproject.jar
				if (subproject.tasks.findByPath("sourcesJar")) {
					from subproject.sourcesJar
				}
				if (subproject.tasks.findByPath("javadocJar")) {
					from subproject.javadocJar
				}
			}
		}
	}

	artifacts {
		archives docsZip
		archives schemaZip
		archives distZip
	}

}

project('spring-data-hadoop-core') {
	description = 'Spring for Apache Hadoop Core'
}

project('spring-data-hadoop-batch') {
	description = 'Spring for Apache Hadoop Batch Features'

	dependencies {
		compile project(":spring-data-hadoop-core")
		compile "org.springframework.batch:spring-batch-core:$springBatchVersion"
		testRuntime "org.springframework.integration:spring-integration-file:$springIntVersion"
	}

}

project('spring-data-hadoop-store') {
	description = 'Spring for Apache Hadoop Store Features'

	configurations {
		testRuntime.exclude group: 'org.apache.hive'
	}

	dependencies {
		compile("org.kitesdk:kite-data-core:$kiteVersion") { dep ->
			exclude group: "log4j", module: "log4j"
		}
		testCompile "org.springframework:spring-messaging:$springVersion"
		testCompile project(":spring-data-hadoop")
		testCompile project(path:":spring-data-hadoop-test", configuration:"testArtifacts")
		testCompile "org.springframework:spring-test:$springVersion"
		testCompile("org.mockito:mockito-core:$mockitoVersion") { dep ->
			exclude group: "org.hamcrest"
		}
		testCompile "org.hamcrest:hamcrest-core:$hamcrestVersion"
		testCompile "org.hamcrest:hamcrest-library:$hamcrestVersion"
		testRuntime "org.xerial.snappy:snappy-java:1.1.0"
	}

}

project('spring-data-hadoop') {
	description = 'Spring for Apache Hadoop Configuration'

	dependencies {
		compile project(":spring-data-hadoop-core")
        compile project(":spring-data-hadoop-batch")
	}

}

project('spring-data-hadoop-test') {
	description = 'Spring for Apache Hadoop Tests Core'

	configurations {
		testCompile.exclude group: 'org.mockito'
		testArtifacts.extendsFrom testRuntime
	}    
	artifacts {
		testArtifacts testJar
	}

	dependencies {
		compile project(":spring-data-hadoop-core")    
		compile "org.springframework:spring-test:$springVersion"
		compile "junit:junit:$junitVersion"
		compile hadoopTestArtifacts
	}   
}


project('spring-data-hadoop-build-tests') {
	apply plugin: 'groovy'
	description = 'Spring for Apache Hadoop Integration Tests'
	configurations {
		patch
	}
	dependencies {
		patch('bouncycastle:bcprov-jdk15:140') 
		patch('org.apache.directory.server:apacheds-core:2.0.0-M15')
		patch('org.apache.directory.server:apacheds-core-api:2.0.0-M15')
		patch('org.apache.directory.server:apacheds-core-avl:2.0.0-M15')
		patch('org.apache.directory.server:apacheds-core-constants:2.0.0-M15')
		patch('org.apache.directory.server:apacheds-core-shared:2.0.0-M15')
		patch('org.apache.directory.server:apacheds-i18n:2.0.0-M15')
		patch('org.apache.directory.server:apacheds-interceptor-kerberos:2.0.0-M15')
		patch('org.apache.directory.server:apacheds-interceptors-admin:2.0.0-M15')
		patch('org.apache.directory.server:apacheds-interceptors-authn:2.0.0-M15')
		patch('org.apache.directory.server:apacheds-interceptors-authz:2.0.0-M15')
		patch('org.apache.directory.server:apacheds-interceptors-changelog:2.0.0-M15')
		patch('org.apache.directory.server:apacheds-interceptors-collective:2.0.0-M15')
		patch('org.apache.directory.server:apacheds-interceptors-event:2.0.0-M15')
		patch('org.apache.directory.server:apacheds-interceptors-exception:2.0.0-M15')
		patch('org.apache.directory.server:apacheds-interceptors-journal:2.0.0-M15')
		patch('org.apache.directory.server:apacheds-interceptors-normalization:2.0.0-M15')
		patch('org.apache.directory.server:apacheds-interceptors-operational:2.0.0-M15')
		patch('org.apache.directory.server:apacheds-interceptors-referral:2.0.0-M15')
		patch('org.apache.directory.server:apacheds-interceptors-schema:2.0.0-M15')
		patch('org.apache.directory.server:apacheds-interceptors-subtree:2.0.0-M15')
		patch('org.apache.directory.server:apacheds-interceptors-trigger:2.0.0-M15')
		patch('org.apache.directory.server:apacheds-jdbm-partition:2.0.0-M15')
		patch('org.apache.directory.server:apacheds-kerberos-codec:2.0.0-M15')
		patch('org.apache.directory.server:apacheds-ldif-partition:2.0.0-M15')
		patch('org.apache.directory.server:apacheds-mavibot-partition:2.0.0-M15')
		patch('org.apache.directory.server:apacheds-protocol-kerberos:2.0.0-M15')
		patch('org.apache.directory.server:apacheds-protocol-ldap:2.0.0-M15')
		patch('org.apache.directory.server:apacheds-protocol-shared:2.0.0-M15')
		patch('org.apache.directory.server:apacheds-xdbm-partition:2.0.0-M15')
		patch('org.apache.directory.api:api-all:1.0.0-M20')
		patch('org.apache.directory.api:api-asn1-api:1.0.0-M20')
		patch('org.apache.directory.api:api-asn1-ber:1.0.0-M20')
		patch('org.apache.directory.api:api-i18n:1.0.0-M20')
		patch('org.apache.directory.api:api-ldap-client-api:1.0.0-M20')
		patch('org.apache.directory.api:api-ldap-codec-core:1.0.0-M20')
		patch('org.apache.directory.api:api-ldap-extras-aci:1.0.0-M20')
		patch('org.apache.directory.api:api-ldap-extras-codec:1.0.0-M20')
		patch('org.apache.directory.api:api-ldap-extras-codec-api:1.0.0-M20')
		patch('org.apache.directory.api:api-ldap-extras-sp:1.0.0-M20')
		patch('org.apache.directory.api:api-ldap-extras-trigger:1.0.0-M20')
		patch('org.apache.directory.api:api-ldap-extras-util:1.0.0-M20')
		patch('org.apache.directory.api:api-ldap-model:1.0.0-M20')
		patch('org.apache.directory.api:api-util:1.0.0-M20')
		patch('net.sf.ehcache:ehcache-core:2.4.4')
		patch('org.apache.directory.mavibot:mavibot:1.0.0-M1')
		patch('org.apache.mina:mina-core:2.0.7')
		patch('org.apache.hadoop:hadoop-minikdc:2.3.0')
		patch('com.googlecode.jarjar:jarjar:1.3')
	}
	project.ext.set("shouldBuildPatch", { !new File('minikdcsecure.jar').exists() })
	task downloadPatchLibs(type: Copy) {
		into('lib')
		from(configurations.patch)
		exclude('jarjar*')
		duplicatesStrategy(DuplicatesStrategy.EXCLUDE)
	}
	downloadPatchLibs.doFirst {
		if(!shouldBuildPatch()) { throw new StopExecutionException() }
	}
	task applyPatch(dependsOn: 'downloadPatchLibs') << {
		if (shouldBuildPatch()) {
	project.ant {
		taskdef name: "jarjar", classname: "com.tonicsystems.jarjar.JarJarTask", classpath: configurations.patch.asPath
		jarjar(jarfile: 'minikdcsecure.jar', filesetmanifest: "merge") {
		zipfileset(src: 'lib/hadoop-minikdc-2.3.0.jar')
		zipfileset(src: 'lib/apacheds-core-2.0.0-M15.jar')
		zipfileset(src: 'lib/apacheds-core-api-2.0.0-M15.jar')
		zipfileset(src: 'lib/apacheds-core-avl-2.0.0-M15.jar')
		zipfileset(src: 'lib/apacheds-core-constants-2.0.0-M15.jar')
		zipfileset(src: 'lib/apacheds-core-shared-2.0.0-M15.jar')
		zipfileset(src: 'lib/apacheds-i18n-2.0.0-M15.jar')
		zipfileset(src: 'lib/apacheds-interceptor-kerberos-2.0.0-M15.jar')
		zipfileset(src: 'lib/apacheds-interceptors-admin-2.0.0-M15.jar')
		zipfileset(src: 'lib/apacheds-interceptors-authn-2.0.0-M15.jar')
		zipfileset(src: 'lib/apacheds-interceptors-authz-2.0.0-M15.jar')
		zipfileset(src: 'lib/apacheds-interceptors-changelog-2.0.0-M15.jar')
		zipfileset(src: 'lib/apacheds-interceptors-collective-2.0.0-M15.jar')
		zipfileset(src: 'lib/apacheds-interceptors-event-2.0.0-M15.jar')
		zipfileset(src: 'lib/apacheds-interceptors-exception-2.0.0-M15.jar')
		zipfileset(src: 'lib/apacheds-interceptors-journal-2.0.0-M15.jar')
		zipfileset(src: 'lib/apacheds-interceptors-normalization-2.0.0-M15.jar')
		zipfileset(src: 'lib/apacheds-interceptors-operational-2.0.0-M15.jar')
		zipfileset(src: 'lib/apacheds-interceptors-referral-2.0.0-M15.jar')
		zipfileset(src: 'lib/apacheds-interceptors-schema-2.0.0-M15.jar')
		zipfileset(src: 'lib/apacheds-interceptors-subtree-2.0.0-M15.jar')
		zipfileset(src: 'lib/apacheds-interceptors-trigger-2.0.0-M15.jar')
		zipfileset(src: 'lib/apacheds-jdbm-partition-2.0.0-M15.jar')
		zipfileset(src: 'lib/apacheds-jdbm1-2.0.0-M2.jar')
		zipfileset(src: 'lib/apacheds-kerberos-codec-2.0.0-M15.jar')
		zipfileset(src: 'lib/apacheds-ldif-partition-2.0.0-M15.jar')
		zipfileset(src: 'lib/apacheds-mavibot-partition-2.0.0-M15.jar')
		zipfileset(src: 'lib/apacheds-protocol-kerberos-2.0.0-M15.jar')
		zipfileset(src: 'lib/apacheds-protocol-ldap-2.0.0-M15.jar')
		zipfileset(src: 'lib/apacheds-protocol-shared-2.0.0-M15.jar')
		zipfileset(src: 'lib/apacheds-xdbm-partition-2.0.0-M15.jar')
		zipfileset(src: 'lib/api-all-1.0.0-M20.jar')
		zipfileset(src: 'lib/api-asn1-api-1.0.0-M20.jar')
		zipfileset(src: 'lib/api-asn1-ber-1.0.0-M20.jar')
		zipfileset(src: 'lib/api-i18n-1.0.0-M20.jar')
		zipfileset(src: 'lib/api-ldap-client-api-1.0.0-M20.jar')
		zipfileset(src: 'lib/api-ldap-codec-core-1.0.0-M20.jar')
		zipfileset(src: 'lib/api-ldap-extras-aci-1.0.0-M20.jar')
		zipfileset(src: 'lib/api-ldap-extras-codec-1.0.0-M20.jar')
		zipfileset(src: 'lib/api-ldap-extras-codec-api-1.0.0-M20.jar')
		zipfileset(src: 'lib/api-ldap-extras-sp-1.0.0-M20.jar')
		zipfileset(src: 'lib/api-ldap-extras-trigger-1.0.0-M20.jar')
		zipfileset(src: 'lib/api-ldap-extras-util-1.0.0-M20.jar')
		zipfileset(src: 'lib/api-ldap-model-1.0.0-M20.jar')
		zipfileset(src: 'lib/api-util-1.0.0-M20.jar')
		zipfileset(src: 'lib/bcprov-jdk15-140.jar'){
		exclude(name: "META-INF/*")
		}
		zipfileset(src: 'lib/ehcache-core-2.4.4.jar')
		zipfileset(src: 'lib/mavibot-1.0.0-M1.jar')
		zipfileset(src: 'lib/mina-core-2.0.7.jar')
	}
	}
	}
}
	task cleanupDownloadPatchLibs(type: Delete, dependsOn: 'applyPatch') {
		delete 'lib/asm-3.1.jar'
	}
	compileGroovy.dependsOn(cleanupDownloadPatchLibs)
	task cleanPatch(type: Delete) {
		delete 'lib'
	}
	processTestResources.dependsOn(cleanPatch)

	dependencies {
		compile files('minikdcsecure.jar')
		compile project(":spring-data-hadoop-core")
		compile project(":spring-data-hadoop-batch")
		compile project(":spring-data-hadoop")
		compile project(":spring-data-hadoop-test")

		// Testing
		testCompile "junit:junit:$junitVersion"
		testCompile("org.mockito:mockito-core:$mockitoVersion") { dep ->
			exclude group: "org.hamcrest"
		}
		testCompile "org.springframework:spring-test:$springVersion"
		testCompile("javax.annotation:jsr250-api:1.0")
		testCompile "org.springframework.integration:spring-integration-stream:$springIntVersion"
		testCompile "org.springframework.integration:spring-integration-file:$springIntVersion"
		testRuntime "org.springframework.integration:spring-integration-event:$springIntVersion"

		testRuntime "cglib:cglib-nodep:$cglibVersion"
		testRuntime "commons-io:commons-io:$commonsioVersion"

		// Testing
		testRuntime "org.codehaus.groovy:groovy:$groovyVersion"
		testRuntime "org.jruby:jruby:$jrubyVersion"
		testRuntime "org.python:jython-standalone:$jythonVersion"
	
		// specify a version of antlr that works with both hive and pig
		testRuntime "org.antlr:antlr-runtime:$antlrVersion"
	}

    task downloadGutenbergBooks {
        ant.get(src: 'http://mirrors.xmission.com/gutenberg/1/0/100/100.txt',
                dest: 'src/test/resources/input/gutenberg',skipexisting:true)
        ant.get(src: 'http://mirrors.xmission.com/gutenberg/1/3/135/135.txt',
                dest: 'src/test/resources/input/gutenberg',skipexisting:true)
        ant.get(src: 'http://mirrors.xmission.com/gutenberg/1/3/9/1399/1399.txt',
                dest: 'src/test/resources/input/gutenberg',skipexisting:true)
        ant.get(src: 'http://mirrors.xmission.com/gutenberg/2/6/0/2600/2600.txt',
                dest: 'src/test/resources/input/gutenberg',skipexisting:true)
    }

	task enablePigTests {
		description = "Enabling Pig tests"
		group = "Verification"
		doLast() {
			project.ext.skipPig = false
		}
	}

	task enableHiveTests {
		description = "Enabling Hive tests"
		group = "Verification"
		doLast() {
			project.ext.skipHive = false
		}
	}

	task enableHBaseTests {
		description = "Enabling HBase tests"
		group = "Verification"
		doLast() {
			project.ext.skipHBase = false
		}
	}

	task enableWebHdfsTests {
		description = "Enabling WebHdfs tests"
		group = "Verification"
		doLast() {
			project.ext.skipWebHdfs = false
		}
	}

	task enableAllTests() {
		description = "Enabling all (incl. Pig, Hive, HBase, WebHdfs) tests"
		group = "Verification"
		doLast() {
			project.ext.skipPig = false
			project.ext.skipHive = false
			project.ext.skipHBase = false
			project.ext.skipWebHdfs = false
		}
	}

	tasks.withType(Test).all {
		if (project.hasProperty('test.forkEvery')) {
			forkEvery = project.getProperty('test.forkEvery').toInteger()
		}
		systemProperties['input.path'] = 'build/classes/test/input'
		systemProperties['output.path'] = 'build/classes/test/output'
		includes = ["**/*.class"]

		testLogging {
			events "started"
			minGranularity 2
			maxGranularity 2
		}

		doFirst() {
			ext.msg = " "

			if (project.ext.skipPig) {
				ext.msg += "Pig "
				excludes.add("**/pig/**")
			}
			if (project.ext.skipHBase) {
				ext.msg += "HBase "
				excludes.add("**/hbase/**")
			}
	
			if (project.ext.skipHive) {
				ext.msg += "Hive "
				excludes.add("**/hive/**")
			}

			if (project.ext.skipWebHdfs) {
				ext.msg += "WebHdfs "
				excludes.add("**/WebHdfs*")
			}

			if (!msg.trim().isEmpty())
					println "Skipping [$msg] Tests";
	
			// check prefix for hd.fs
			// first copy the properties since we can't change them
			ext.projProps = project.properties
	
			if (projProps.containsKey("hd.fs")) {
					String hdfs = projProps["hd.fs"].toString()
					if (!hdfs.contains("://")) {
						projProps.put("hd.fs", "hdfs://" + hdfs)
					}
			}
			
			// due to GRADLE-2475, set the system properties manually
			projProps.each { k,v ->
					if (k.toString().startsWith("hd.")) {
						systemProperties[k] = projProps[k]
					}
			}
		}
	}

}

if (gradle.ext.mr2) {

	configure(yarnProjects()) {
		task integrationTest(type: Test) {
			include '**/*IntegrationTests.*'
		}
		tasks.withType(Test).all {
			exclude '**/*IntegrationTests.*'
		}
		dependencies {
			testCompile "org.springframework:spring-test:$springVersion"
			testCompile "org.hamcrest:hamcrest-core:$hamcrestVersion"
			testCompile "org.hamcrest:hamcrest-library:$hamcrestVersion"
			testCompile "junit:junit:$junitVersion"
		}
		clean.doLast {ant.delete(dir: "target")}
	}

	project('spring-yarn') {
		description = 'Spring for Apache Hadoop YARN'
		dependencies {
			compile project("spring-yarn-batch")
		}
	}

	project('spring-yarn:spring-yarn-core') {
		description = 'Spring Yarn Core'
		dependencies {
			compile project(":spring-data-hadoop")
			compile("org.apache.hadoop:hadoop-yarn-client:$hadoopVersion") { dep ->
				exclude group: "org.slf4j", module: "slf4j-log4j12"
			}
			compile("org.apache.hadoop:hadoop-common:$hadoopVersion") { dep ->
				exclude group: "junit", module: "junit"
			}
			testCompile("org.mockito:mockito-core:$mockitoVersion") { dep ->
				exclude group: "org.hamcrest"
			}
		}

		tasks.withType(Test).all {

			doFirst() {
				// check prefix for hd.fs
				// first copy the properties since we can't change them
				ext.projProps = project.properties
	
				if (projProps.containsKey("hd.fs")) {
						String hdfs = projProps["hd.fs"].toString()
						if (!hdfs.contains("://")) {
							projProps.put("hd.fs", "hdfs://" + hdfs)
						}
				}
			
				// due to GRADLE-2475, set the system properties manually
				projProps.each { k,v ->
						if (k.toString().startsWith("hd.")) {
							systemProperties[k] = projProps[k]
						}
						if (k.toString().equals("profiles")) {
							systemProperties['spring.profiles.active'] = projProps[k]
						}
				}
			}

		}
	}

	project('spring-yarn:spring-yarn-integration') {
		description = 'Spring Yarn Integration'
		dependencies {
			compile project(":spring-yarn:spring-yarn-core")
			compile "org.springframework.integration:spring-integration-ip:$springIntVersion"
			compile "com.fasterxml.jackson.core:jackson-core:$jackson2Version"
			compile "com.fasterxml.jackson.core:jackson-databind:$jackson2Version"
			testCompile "org.springframework.integration:spring-integration-test:$springIntVersion"
		}
	}

	project('spring-yarn:spring-yarn-batch') {
		description = 'Spring Yarn Batch'
		dependencies {
			compile project(":spring-yarn:spring-yarn-integration")
			compile project(":spring-data-hadoop-store")
			compile "org.springframework.batch:spring-batch-core:$springBatchVersion"
			compile "org.springframework.batch:spring-batch-infrastructure:$springBatchVersion"
			testCompile project(":spring-data-hadoop-core")
			testCompile project(":spring-yarn:spring-yarn-test")
		}
	}

	project('spring-yarn:spring-yarn-boot') {
		description = 'Spring Yarn Boot'
		dependencies {
			compile project(":spring-yarn:spring-yarn-core")
			provided project(":spring-yarn:spring-yarn-batch")
			provided "org.springframework:spring-web:$springVersion"
			compile "org.springframework.boot:spring-boot-autoconfigure:$springBootVersion"
			runtime "org.yaml:snakeyaml:$snakeYamlVersion"
		}
	}

	project('spring-yarn:spring-yarn-boot-build-tests') {

		apply plugin: 'spring-boot'

		description = 'Spring Yarn Boot'
		dependencies {
			compile project(":spring-yarn:spring-yarn-boot")
			testCompile project(":spring-yarn:spring-yarn-boot")
			testCompile project(":spring-yarn:spring-yarn-test")
		}

		// create a boot jar which we can use in tests
		// disable main bootRepackage task so that it
		// doesn't mess with main artifact
		// test needs to depend on these tasks
		task appmasterJar(type: Jar) {
			archiveName = 'test-archive-appmaster.jar'
			from sourceSets.test.output
		}
		task appmasterBootJar(type: BootRepackage, dependsOn: appmasterJar) {
			withJarTask = appmasterJar
			mainClass = 'org.springframework.yarn.boot.app.SpringYarnBootApplication'
		}
		bootRepackage.enabled = false
		tasks.withType(Test).all { dependsOn(appmasterBootJar) }

		// boot plugin imports gradle application plugin
		// which creates distZip task which fails
		// because we don't need mainClassname, so
		// disabling startScripts and distZip tasks
		startScripts.enabled = false
		distZip.enabled = false
	}

	project('spring-yarn:spring-yarn-test') {
		description = 'Spring Yarn Test Core'
		configurations {
			hadoopruntime.exclude group: 'log4j'
			hadoopruntime.exclude group: 'org.slf4j'
			hadoopruntime.exclude group: 'org.apache.hadoop'
			hadoopruntime.exclude group: 'commons-logging'
			hadoopruntime.exclude group: 'org.codehaus.jettison'
			hadoopruntime.exclude group: 'com.thoughtworks.xstream'
			hadoopruntimenotest.exclude group: 'org.apache.hadoop', module: 'hadoop-yarn-server-tests'
		}
		dependencies {
            compile project(":spring-yarn:spring-yarn-core")
			compile "org.springframework:spring-test:$springVersion"
			compile "junit:junit:$junitVersion"
			compile("org.apache.hadoop:hadoop-yarn-client:$hadoopVersion") { dep ->
				exclude group: "org.slf4j", module: "slf4j-log4j12"
			}
			compile("org.apache.hadoop:hadoop-common:$hadoopVersion") { dep ->
				exclude group: "junit", module: "junit"
			}
			compile("org.apache.hadoop:hadoop-yarn-server-tests:$hadoopVersion") { dep ->
				exclude group: "org.slf4j", module: "slf4j-log4j12"
			}
			compile("org.apache.hadoop:hadoop-yarn-server-tests:$hadoopVersion:tests") { dep ->
				exclude group: "org.slf4j", module: "slf4j-log4j12"
			}
			compile "org.apache.hadoop:hadoop-hdfs:$hadoopVersion"
			compile("org.apache.hadoop:hadoop-hdfs:$hadoopVersion:tests") { dep ->
				exclude group: "log4j", module: "log4j"
			}
			compile("org.apache.hadoop:hadoop-common:$hadoopVersion:tests") { dep ->
				exclude group: "log4j", module: "log4j"
				exclude group: "org.slf4j", module: "slf4j-log4j12"
			}
			hadoopruntime configurations.runtime
			hadoopruntimenotest configurations.runtime
		}
		task copyHadoopRuntimeDeps(type: Copy) {
			into "$buildDir/dependency-libs"
			from configurations.hadoopruntime
		}
		task copyHadoopRuntimeDepsAll(type: Copy) {
			into "$buildDir/dependency-all-libs"
			from configurations.hadoopruntimenotest
		}
		tasks.withType(Test).all { dependsOn([copyHadoopRuntimeDeps,copyHadoopRuntimeDepsAll]) }
	}

	project('spring-yarn:spring-yarn-build-tests') {
		description = 'Spring Yarn Integration Test'
		configurations {
			hadoopruntime.exclude group: 'log4j'
			hadoopruntime.exclude group: 'org.slf4j'
			hadoopruntime.exclude group: 'org.apache.hadoop'
			hadoopruntime.exclude group: 'commons-logging'
			hadoopruntime.exclude group: 'org.codehaus.jettison'
			hadoopruntime.exclude group: 'com.thoughtworks.xstream'
			hadoopruntimenotest.exclude group: 'org.apache.hadoop', module: 'hadoop-yarn-server-tests'
		}
		dependencies {
			compile project(":spring-yarn:spring-yarn-core")
			compile project(":spring-yarn:spring-yarn-test")
			hadoopruntime configurations.runtime
			hadoopruntimenotest configurations.runtime
		}
		task copyHadoopRuntimeDeps(type: Copy) {
			into "$buildDir/dependency-libs"
			from configurations.hadoopruntime
		}
		task copyHadoopRuntimeDepsAll(type: Copy) {
			into "$buildDir/dependency-all-libs"
			from configurations.hadoopruntimenotest
		}
		tasks.withType(Test).all { dependsOn([copyHadoopRuntimeDeps,copyHadoopRuntimeDepsAll]) }
	}

	project('spring-yarn:spring-yarn-boot-test') {
		description = 'Spring Yarn Boot Test'
		dependencies {
			compile project(":spring-yarn:spring-yarn-boot")
			compile project(":spring-yarn:spring-yarn-test")
		}
	}

}

task wrapper(type: Wrapper) {
	description = "Generates gradlew[.bat] scripts"
	gradleVersion = "1.8"
}

