description = 'Spring for Apache Hadoop'

defaultTasks 'build'

buildscript {
	repositories {
		maven { url "http://repo.springsource.org/plugins-release" }
	}
	dependencies {
		classpath("org.springframework.build.gradle:propdeps-plugin:0.0.3")
		classpath("org.springframework.build.gradle:docbook-reference-plugin:0.2.6")
	}
}

allprojects {
	group = 'org.springframework.data'

	repositories {
		mavenCentral()
		maven { url 'http://repo.springsource.org/libs-milestone' }
	}
}

def javaProjects() {
	subprojects.findAll { project -> project.name != 'docs' }
}

def hadoopProjects() {
	subprojects.findAll { project -> project.name.contains('-hadoop-') || project.name.contains('-cascading') }
}

def yarnProjects() {
	subprojects.findAll { project -> project.name.contains('-yarn-') }
}

// 
//  Select the Hadoop distribution used for building the binaries
// 
def List hadoop = []

def hadoopDefault = "hadoop12"
def hadoopDistro = project.hasProperty("distro") ? project.getProperty("distro") : hadoopDefault
def hadoopVersion = hd12Version

// make it possible to use Pig jars compiled for Hadoop 2.0
def pigQualifier = ''

// handle older Hive version
def hiveGroup = "org.apache.hive"

// default is Hadoop 1.2.x
switch (hadoopDistro) {

	// Cloudera CDH3
	case "cdh3":
		hadoopVersion = cdh3Version
		println "Using Cloudera CDH3 [$hadoopVersion]"
		hbaseVersion = cdh3HbaseVersion
		// Hive in CDH3 is too old and does not allow Hive Server to be compiled
		// Note that the POMs and repo are incomplete (hive-builtin is missing)
		// hiveVersion = cdh3HiveVersion
		// hiveGroup = "org.apache.hadoop.hive"
		pigVersion = cdh3PigVersion
		break;	

	// Cloudera CDH4
	case "cdh4":
		hadoopVersion = cdh4MR1Version
		println "Using Cloudera CDH4 [$hadoopVersion]"
		hbaseVersion = cdh4HbaseVersion
		hiveVersion = cdh4HiveVersion
		pigVersion = cdh4PigVersion
		thriftVersion = cdh4ThriftVersion
		break;	

	// Pivotal HD 1.0
	case "phd1":
		hadoopVersion = phd1Version
		println "Using Pivotal HD 1.0 - [$hadoopVersion]"
		hbaseVersion = phd1HbaseVersion
		hiveVersion = phd1HiveVersion
		pigVersion = phd1PigVersion
		thriftVersion = phd1ThriftVersion
		hadoop = ["org.apache.hadoop:hadoop-common:$hadoopVersion",
			  "org.apache.hadoop:hadoop-hdfs:$hadoopVersion",
			  "org.apache.hadoop:hadoop-mapreduce-client-core:$hadoopVersion",
			  "org.apache.hadoop:hadoop-streaming:$hadoopVersion",
			  "org.apache.hadoop:hadoop-distcp:$hadoopVersion"]
		break;

	// Hortonworks Data Platform 1.3
	case "hdp13":
		hadoopVersion = hdp13Version
		println "Using Hortonworks Data Platform 1.3 [$hadoopVersion]"
		hadoop = ["org.apache.hadoop:hadoop-streaming:$hadoopVersion",
			"org.apache.hadoop:hadoop-tools:$hadoopVersion"]
		hbaseVersion = hdp13HbaseVersion
		hiveVersion = hdp13HiveVersion
		pigVersion = hdp13PigVersion
		thriftVersion = hdp13ThriftVersion
		break;

	// Hadoop 2.0 Alpha
	case "hadoop20":
		hadoopVersion = hd20Version
		println "Using Apache Hadoop 2.0 - [$hadoopVersion]"
		hadoop = ["org.apache.hadoop:hadoop-common:$hadoopVersion",
			  "org.apache.hadoop:hadoop-hdfs:$hadoopVersion",
			  "org.apache.hadoop:hadoop-mapreduce-client-core:$hadoopVersion",
			  "org.apache.hadoop:hadoop-streaming:$hadoopVersion",
			  "org.apache.hadoop:hadoop-distcp:$hadoopVersion"]
		hbaseVersion = hd20HbaseVersion
		hiveVersion = hd20HiveVersion
		pigVersion = hd20PigVersion
		pigQualifier = ':h2'
		thriftVersion = hd20ThriftVersion
		break;

	// Hadoop 1.1.x
	case "hadoop11":
		hadoopVersion = hd11Version
		println "Using Apache Hadoop 1.1.x [$hadoopVersion]"
		hadoop = ["org.apache.hadoop:hadoop-streaming:$hadoopVersion",
			  "org.apache.hadoop:hadoop-tools:$hadoopVersion"]
		break;

	// Hadoop 1.0.x
	case "hadoop10":
		hadoopVersion = hd10Version
		println "Using Apache Hadoop 1.0.x [$hadoopVersion]"
		hadoop = ["org.apache.hadoop:hadoop-streaming:$hadoopVersion",
			  "org.apache.hadoop:hadoop-tools:$hadoopVersion"]
		break;

	default:
		if (!project.hasProperty("distro")) {
			println "Using default distro: Apache Hadoop [$hadoopVersion]"
		} else {
			if (hadoopDistro == hadoopDefault) {
				println "Using Apache Hadoop 1.2.x [$hadoopVersion]"
			} else {
				println "Failing build: $hadoopDistro is not a supported distro"
				println "Supported distros: hadoop10, hadoop11, hadoop12[*], hadoop20, hdp13, chd3, chd4 and phd1"
				println "* default"
				throw new InvalidUserDataException("$hadoopDistro is not a supported distro")
			}
		}
		hadoopVersion = hd12Version
		hadoop = ["org.apache.hadoop:hadoop-streaming:$hadoopVersion",
			"org.apache.hadoop:hadoop-tools:$hadoopVersion"]
		hbaseVersion = hd12HbaseVersion
		hiveVersion = hd12HiveVersion
		pigVersion = hd12PigVersion
		thriftVersion = hd12ThriftVersion
}

configure(javaProjects()) {

	apply plugin: 'java'
	apply from:   "${rootProject.projectDir}/maven.gradle"
	apply plugin: 'eclipse'
	apply plugin: 'idea'
	apply plugin: 'propdeps'
	apply plugin: 'propdeps-idea'
	apply plugin: 'propdeps-eclipse'

	sourceCompatibility=1.6
	targetCompatibility=1.6

	// assume we are skipping these tests (must be enabled explicitly)
	ext.skipPig = true
	ext.skipHive = true
	ext.skipHBase = true
	ext.skipWebHdfs = true
	ext.skipCascading = true

	// exclude poms from the classpath (pulled in by Cloudera)
	eclipse.classpath.file {
		whenMerged { classpath ->
			classpath.entries.removeAll { entry -> entry.toString().contains(".pom") }
		}
	}

	eclipse {
		project {
			natures += 'org.springframework.ide.eclipse.core.springnature'
		}
	}

	// dependencies that are common across all java projects
	dependencies {
		compile "org.springframework:spring-aop:$springVersion"
		compile "org.springframework:spring-context:$springVersion"
		compile "org.springframework:spring-context-support:$springVersion"
		compile "org.springframework:spring-jdbc:$springVersion"
		compile "org.springframework:spring-tx:$springVersion"
	}

	task sourcesJar(type: Jar) {
		classifier = 'sources'
		from sourceSets.main.allJava
	}

	task testJar(type: Jar) {
		classifier = 'tests'
		from sourceSets.test.output
	}

	task javadocJar(type: Jar) {
		classifier = 'javadoc'
		from javadoc
	}

	artifacts {
		archives sourcesJar
		archives javadocJar
	}

	assemble.dependsOn = ['jar', 'sourcesJar', 'testJar']

	javadoc {
		ext.srcDir = file("${projectDir}/docs/src/api")
	  
		configure(options) {
			stylesheetFile = file("${rootProject.projectDir}/docs/src/api/spring-javadoc.css")
			overview = "${rootProject.projectDir}/docs/src/api/overview.html"
			docFilesSubDirs = true
			outputLevel = org.gradle.external.javadoc.JavadocOutputLevel.QUIET
			breakIterator = true
			author = true
			showFromProtected()

	//		groups = [
	//		'Spring Data Hadoop' : ['org.springframework.data.hadoop*'],
	//		]
  
			links = [
				"http://static.springframework.org/spring/docs/3.0.x/javadoc-api",
				"http://download.oracle.com/javase/6/docs/api",
				"http://commons.apache.org/proper/commons-logging/commons-logging-1.1.1/apidocs/",
				"http://logging.apache.org/log4j/1.2/apidocs/",
				"http://hadoop.apache.org/common/docs/current/api/",
				"http://hbase.apache.org/apidocs/",
				"http://pig.apache.org/docs/r0.10.0/api/",
				"http://hive.apache.org/docs/r0.7.1/api/",
				"http://static.springsource.org/spring-batch/apidocs/",
				"http://static.springsource.org/spring-integration/api/",
				"https://builds.apache.org/job/Thrift/javadoc/",
				"http://docs.cascading.org/cascading/2.1/javadoc/"
			]

			exclude "org/springframework/data/hadoop/config/**"
	  	}

		title = "${rootProject.description} ${version} API"
	}

	jar {
		manifest.attributes["Created-By"] = "${System.getProperty("java.version")} (${System.getProperty("java.specification.vendor")})"
		manifest.attributes['Implementation-Title'] = 'spring-data-hadoop'
		manifest.attributes['Implementation-Version'] = project.version
		manifest.attributes['Implementation-URL'] = "http://www.springsource.org/spring-data/hadoop"
		manifest.attributes['Implementation-Vendor'] = "SpringSource"
		manifest.attributes['Implementation-Vendor-Id'] = "org.springframework"

		def build = System.env['SHDP.BUILD']
		if (build != null)
			manifest.attributes['Build'] = build

		String rev = "unknown"

		// parse the git files to find out the revision
		File gitHead = file('.git/HEAD')
		if (gitHead.exists()) {
			gitHead = file('.git/' + gitHead.text.trim().replace('ref: ',''))
			if (gitHead.exists()) { rev = gitHead.text }
		}

		from("$rootDir/docs/src/info") {
			include "license.txt"
			include "notice.txt"
			into "META-INF"
			expand(copyright: new Date().format('yyyy'), version: project.version)
		}

		manifest.attributes['Repository-Revision'] = rev
	}

}

configure(hadoopProjects()) {

	// default is Hadoop 1.2.x
	switch (hadoopDistro) {

		// Cloudera CDH3
		case "cdh3":
			dependencies {
				optional("org.apache.hadoop:hadoop-streaming:$hadoopVersion") 
				optional("org.apache.hadoop:hadoop-tools:$hadoopVersion")
				testCompile "org.apache.hadoop:hadoop-examples:$hadoopVersion"
			}
			break;	

		// Cloudera CDH4
		case "cdh4":
			dependencies {
				optional("org.apache.hadoop:hadoop-streaming:$cdh4MR1Version") 
				optional("org.apache.hadoop:hadoop-tools:$cdh4MR1Version")
				optional("org.apache.hadoop:hadoop-common:$cdh4Version")
				optional("org.apache.hadoop:hadoop-hdfs:$cdh4Version")
				testCompile "org.apache.hadoop:hadoop-examples:$hadoopVersion"
			}
			break;	

		// Pivotal HD 2.0
		case "phd1":
			dependencies {
				compile "org.apache.hive:hive-common:$hiveVersion"
				compile "org.apache.hive:hive-metastore:$hiveVersion"
				compile "org.apache.hive:hive-exec:$hiveVersion"
				testCompile "org.apache.hadoop:hadoop-mapreduce-examples:$hadoopVersion"
				testRuntime "org.apache.hadoop:hadoop-mapreduce-client-jobclient:$hadoopVersion"
			}
			break;

		// Hortonworks Data Platform 1.3
		case "hdp13":
			dependencies {
				testCompile "org.apache.hadoop:hadoop-examples:$hadoopVersion"
				testRuntime "dk.brics.automaton:automaton:1.11-8"
			}
			break;

		// Hadoop 2.0 Alpha
		case "hadoop20":
			dependencies {
				testCompile "org.apache.hadoop:hadoop-mapreduce-examples:$hadoopVersion"
				testRuntime "org.apache.hadoop:hadoop-mapreduce-client-jobclient:$hadoopVersion"
				testRuntime "dk.brics.automaton:automaton:1.11-8"
			}
			break;

		// Hadoop 1.1.x
		case "hadoop11":
			dependencies {
				testCompile "org.apache.hadoop:hadoop-examples:$hadoopVersion"
			}
			break;

		// Hadoop 1.0.x
		case "hadoop10":
			dependencies {
				testCompile "org.apache.hadoop:hadoop-examples:$hadoopVersion"
			}
			break;

		default:
			dependencies {
				testCompile "org.apache.hadoop:hadoop-examples:$hadoopVersion"
			}
	}

	dependencies {
		compile hadoop

		// Logging - using commons-logging from spring-core
		testRuntime("log4j:log4j:$log4jVersion")

		// Spring Framework
		// context-support -> spring-aop/beans/core -> commons-logging
		compile "org.springframework:spring-context-support:$springVersion"
		// used for DAO exceptions by Pig/HBase/Hive packages
		optional("org.springframework:spring-tx:$springVersion")
		// used by Hive package
		optional("org.springframework:spring-jdbc:$springVersion")
	
		// Missing dependency in Hadoop 1.0.3
		testRuntime "commons-io:commons-io:$commonsioVersion"
		testRuntime "org.codehaus.jackson:jackson-mapper-asl:$jacksonVersion"
		testRuntime "cglib:cglib:$cglibVersion"

		// Hive
		optional("$hiveGroup:hive-service:$hiveVersion")
	
		// needed by JDBC test
		testRuntime "$hiveGroup:hive-jdbc:$hiveVersion"
	
		// needed by the Hive Server tests
		// testRuntime "$hiveGroup:hive-builtins:$hiveVersion" 
		// testRuntime("$hiveGroup:hive-metastore:$hiveVersion") 

		//testRuntime "$hiveGroup:hive-common:$hiveVersion"
		//testRuntime "$hiveGroup:hive-shims:$hiveVersion"
		//testRuntime "$hiveGroup:hive-serde:$hiveVersion"
		//testRuntime "org.apache.thrift:libthrift:$thriftVersion"
		//testRuntime "org.apache.thrift:libfb303:$thriftVersion"

		// Pig
		optional("org.apache.pig:pig:$pigVersion$pigQualifier") { dep ->
			exclude module: "junit"
		}
	
		// HBase
		optional("org.apache.hbase:hbase:$hbaseVersion") { dep ->
			exclude module: "thrift"
		}

		// Libs dependencies (specified to cope with incompatibilities between them)
		// testRuntime "org.antlr:antlr:$antlrVersion"
		// testRuntime "org.antlr:antlr-runtime:$antlrVersion"

	
		// Testing
		testCompile "junit:junit:$junitVersion"
	}

}

configure(rootProject) {

	apply plugin: 'eclipse'
	apply plugin: 'idea'

	apply plugin: "docbook-reference"

	reference {
		sourceDir = file("docs/src/reference/docbook")
		pdfFilename = "spring-data-hadoop-reference.pdf"
	}

	// don't publish the default jar for the root project
	configurations.archives.artifacts.clear()

	task api(type: Javadoc) {
		group = "Documentation"
		description = "Generates aggregated Javadoc API documentation."
		title = "${rootProject.description} ${version} API"

		dependsOn {
			subprojects.collect {
				it.tasks.getByName("jar")
			}
		}
		options.memberLevel = org.gradle.external.javadoc.JavadocMemberLevel.PROTECTED
		options.author = true
		options.header = rootProject.description
		options.overview = "docs/src/api/overview.html"
		options.stylesheetFile = file("docs/src/api/spring-javadoc.css")
		options.splitIndex = true
		//options.links(project.ext.javadocLinks)

		source subprojects.collect { project ->
			project.sourceSets.main.allJava
		}

		maxMemory = "1024m"
		destinationDir = new File(buildDir, "api")

		doFirst {
			classpath = files(subprojects.collect { it.sourceSets.main.compileClasspath })
		}
	}

	task docsZip(type: Zip) {
		group = "Distribution"
		baseName = "spring-data-hadoop"
		classifier = "docs"
		description = "Builds -${classifier} archive containing api and reference " +
			"for deployment at http://static.springframework.org/spring-hadoop/docs."

		from("docs/src/info") {
			include "changelog.txt"
		}

		from (api) {
			into "javadoc-api"
		}

		from (reference) {
			into "spring-data-hadoop-reference"
		}
	}

	task schemaZip(type: Zip) {
		group = "Distribution"
		baseName = "spring-framework"
		classifier = "schema"
		description = "Builds -${classifier} archive containing all " +
			"XSDs for deployment at http://springframework.org/schema."

		subprojects.each { subproject ->
			def Properties schemas = new Properties();

			subproject.sourceSets.main.resources.find {
				it.path.endsWith("META-INF/spring.schemas")
			}?.withInputStream { schemas.load(it) }

			for (def key : schemas.keySet()) {
				def shortName = key.replaceAll(/http.*schema.(.*).spring-.*/, '$1')
				assert shortName != key
				File xsdFile = subproject.sourceSets.main.resources.find {
					it.path.endsWith(schemas.get(key))
				}
				assert xsdFile != null
				into (shortName) {
					from xsdFile.path
				}
			}
		}
	}

	task distZip(type: Zip, dependsOn: [docsZip, schemaZip]) {
		group = "Distribution"
		baseName = "spring-framework"
		classifier = "dist"
		description = "Builds -${classifier} archive, containing all jars and docs, " +
					"suitable for community download page."

		ext.baseDir = "${baseName}-${project.version}";

		from("docs/src/info") {
			include "readme.txt"
			include "license.txt"
			include "notice.txt"
			into "${baseDir}"
			expand(copyright: new Date().format("yyyy"), version: project.version)
		}

		from(zipTree(docsZip.archivePath)) {
			into "${baseDir}/docs"
		}

		from(zipTree(schemaZip.archivePath)) {
			into "${baseDir}/schema"
		}

		subprojects.each { subproject ->
			into ("${baseDir}/libs") {
				from subproject.jar
				if (subproject.tasks.findByPath("sourcesJar")) {
					from subproject.sourcesJar
				}
				if (subproject.tasks.findByPath("javadocJar")) {
					from subproject.javadocJar
				}
			}
		}
	}

	artifacts {
		archives docsZip
		archives schemaZip
		archives distZip
	}

}

project('spring-data-hadoop-core') {
	description = 'Spring for Apache Hadoop Core'
}

project('spring-data-hadoop-batch') {
	description = 'Spring for Apache Hadoop Batch Features'

	dependencies {
		compile project(":spring-data-hadoop")
		compile "org.springframework.batch:spring-batch-core:$springBatchVersion"
		testRuntime "org.springframework.integration:spring-integration-file:$springIntVersion"
	}

}

project('spring-data-hadoop') {
	description = 'Spring for Apache Hadoop Configuration'

	dependencies {
		compile project(":spring-data-hadoop-core")
	}

}

project('spring-hadoop-test') {
	description = 'Spring for Apache Hadoop Integration Tests'

	dependencies {
		compile project(":spring-data-hadoop-core")
		compile project(":spring-data-hadoop-batch")
		compile project(":spring-data-hadoop")
		compile project(":spring-cascading")

		// Testing
		testCompile "junit:junit:$junitVersion"
		testCompile("org.mockito:mockito-core:$mockitoVersion") { dep ->
			exclude group: "org.hamcrest"
		}
		testCompile "org.springframework:spring-test:$springVersion"
		testCompile("javax.annotation:jsr250-api:1.0")
		testCompile "org.springframework.integration:spring-integration-stream:$springIntVersion"
		testCompile "org.springframework.integration:spring-integration-file:$springIntVersion"
		testRuntime "org.springframework.integration:spring-integration-event:$springIntVersion"

		testRuntime "cglib:cglib:$cglibVersion"
		testRuntime "commons-io:commons-io:$commonsioVersion"

		testCompile "cascading:cascading-local:$cascadingVersion"

		// Testing
		testRuntime "org.codehaus.groovy:groovy:$groovyVersion"
		testRuntime "org.jruby:jruby:$jrubyVersion"
		testRuntime "org.python:jython-standalone:$jythonVersion"
	
		// specify a version of antlr that works with both hive and pig (works only during compilation)
		testRuntime "org.antlr:antlr-runtime:$antlrVersion"
	}

	task downloadGutenbergBooks {
		ant.get(src: 'http://www.gutenberg.lib.md.us/1/0/100/100.txt', 
				dest: 'src/test/resources/input/gutenberg',skipexisting:true)
		ant.get(src: 'http://www.gutenberg.lib.md.us/1/3/135/135.txt', 
				dest: 'src/test/resources/input/gutenberg',skipexisting:true)
		ant.get(src: 'http://www.gutenberg.lib.md.us/1/3/9/1399/1399.txt', 
				dest: 'src/test/resources/input/gutenberg',skipexisting:true)
		ant.get(src: 'http://www.gutenberg.lib.md.us/2/6/0/2600/2600.txt', 
				dest: 'src/test/resources/input/gutenberg',skipexisting:true)
	}

	task enablePigTests {
		description = "Enabling Pig tests"
		group = "Verification"
		doLast() {
			project.ext.skipPig = false
		}
	}

	task enableHiveTests {
		description = "Enabling Hive tests"
		group = "Verification"
		doLast() {
			project.ext.skipHive = false
		}
	}

	task enableHBaseTests {
		description = "Enabling HBase tests"
		group = "Verification"
		doLast() {
			project.ext.skipHBase = false
		}
	}

	task enableWebHdfsTests {
		description = "Enabling WebHdfs tests"
		group = "Verification"
		doLast() {
			project.ext.skipWebHdfs = false
		}
	}

	task enableCascadingTests {
		description = "Enabling Cascading tests"
		group = "Verification"
		doLast() {
			project.ext.skipCascading = false
		}
	}

	task enableAllTests() {
		description = "Enabling all (incl. Pig, Hive, HBase, WebHdfs, Cascading) tests"
		group = "Verification"
		doLast() {
			project.ext.skipPig = false
			project.ext.skipHive = false
			project.ext.skipHBase = false
			project.ext.skipWebHdfs = false
			project.ext.skipCascading = false
		}
	}

	test {
		if (project.hasProperty('test.forkEvery')) {
			forkEvery = project.getProperty('test.forkEvery').toInteger()
		}
		systemProperties['input.path'] = 'build/classes/test/input'
		systemProperties['output.path'] = 'build/classes/test/output'
		includes = ["**/*.class"]

		testLogging {
			events "started"
			minGranularity 2
			maxGranularity 2
		}

		doFirst() {
			ext.msg = " "

			if (project.ext.skipPig) {
				ext.msg += "Pig "
				excludes.add("**/pig/**")
			}
			if (project.ext.skipHBase) {
				ext.msg += "HBase "
				excludes.add("**/hbase/**")
			}
	
			if (project.ext.skipHive) {
				ext.msg += "Hive "
				excludes.add("**/hive/**")
			}

			if (project.ext.skipWebHdfs) {
				ext.msg += "WebHdfs "
				excludes.add("**/WebHdfs*")
			}

			if (project.ext.skipCascading) {
				ext.msg += "Cascading "
				excludes.add("**/cascading/**")
			}
	
			if (!msg.trim().isEmpty())
					println "Skipping [$msg] Tests";
	
			// check prefix for hd.fs
			// first copy the properties since we can't change them
			ext.projProps = project.properties
	
			if (projProps.containsKey("hd.fs")) {
					String hdfs = projProps["hd.fs"].toString()
					if (!hdfs.contains("://")) {
						projProps.put("hd.fs", "hdfs://" + hdfs)
					}
			}
			
			// due to GRADLE-2475, set the system properties manually
			projProps.each { k,v ->
					if (k.toString().startsWith("hd.")) {
						systemProperties[k] = projProps[k]
					}
			}
		}
	}

}

project('spring-cascading') {
	description = 'Spring Cascading Support'

	dependencies {
		compile project(":spring-data-hadoop-batch")

		compile "org.springframework.integration:spring-integration-core:$springIntVersion"
		// cascading
		compile("cascading:cascading-hadoop:$cascadingVersion") { dep ->
			exclude module: "hadoop-core"
		}

	}

}

if (gradle.ext.mr2) {

	configure(yarnProjects()) {
		task integrationTest(type: Test) {
			include '**/*IntegrationTests.*'
		}
		test {
			exclude '**/*IntegrationTests.*'
		}
		dependencies {
			testCompile "org.springframework:spring-test:$springVersion"
			testCompile "org.hamcrest:hamcrest-core:$hamcrestVersion"
			testCompile "org.hamcrest:hamcrest-library:$hamcrestVersion"
			testCompile "junit:junit:$junitVersion"
		}
		clean.doLast {ant.delete(dir: "target")}
	}

	project('spring-yarn') {
		description = 'Spring for Apache Hadoop YARN'	
		dependencies {
			compile project("spring-yarn-batch")
		}
	}
	
	project('spring-yarn:spring-yarn-core') {
		description = 'Spring Yarn Core'
		dependencies {
			compile "org.apache.hadoop:hadoop-yarn-client:$hadoopVersion"
			compile("org.apache.hadoop:hadoop-common:$hadoopVersion") { dep ->
				exclude module: "junit"
			}
			testCompile("org.mockito:mockito-core:$mockitoVersion") { dep ->
				exclude group: "org.hamcrest"
			}
		}
	}

	project('spring-yarn:spring-yarn-integration') {
		description = 'Spring Yarn Integration'
		dependencies {
			compile project(":spring-yarn:spring-yarn-core")
			compile "org.springframework.integration:spring-integration-ip:$springIntVersion"
			compile "com.fasterxml.jackson.core:jackson-core:$jackson2Version"
			compile "com.fasterxml.jackson.core:jackson-databind:$jackson2Version"
			testCompile "org.springframework.integration:spring-integration-test:$springIntVersion"
		}
	}

	project('spring-yarn:spring-yarn-batch') {
		description = 'Spring Yarn Batch'
		dependencies {
			compile project(":spring-yarn:spring-yarn-integration")
			compile "org.springframework.batch:spring-batch-core:$springBatchVersion"
			compile "org.springframework.batch:spring-batch-infrastructure:$springBatchVersion"
			testCompile project(":spring-data-hadoop-core")
		}
	}

	project('spring-yarn:spring-yarn-test-core') {
		description = 'Spring Yarn Test Core'
		configurations {
			hadoopruntime.exclude group: 'log4j'
			hadoopruntime.exclude group: 'org.slf4j'
			hadoopruntime.exclude group: 'org.apache.hadoop'
			hadoopruntime.exclude group: 'commons-logging'
			hadoopruntime.exclude group: 'org.codehaus.jettison'
			hadoopruntime.exclude group: 'com.thoughtworks.xstream'
		}
		dependencies {
			compile "org.springframework:spring-test:$springVersion"
			compile "junit:junit:$junitVersion"
			compile "org.apache.hadoop:hadoop-yarn-client:$hadoopVersion"
			compile("org.apache.hadoop:hadoop-common:$hadoopVersion") { dep ->
				exclude module: "junit"
			}
			compile "org.apache.hadoop:hadoop-yarn-server-tests:$hadoopVersion"
			compile "org.apache.hadoop:hadoop-yarn-server-tests:$hadoopVersion:tests"
			compile "org.apache.hadoop:hadoop-hdfs:$hadoopVersion"
			compile "org.apache.hadoop:hadoop-hdfs:$hadoopVersion:tests"
			compile "org.apache.hadoop:hadoop-common:$hadoopVersion:tests"
			hadoopruntime configurations.runtime
		}
		task copyHadoopRuntimeDeps(type: Copy) {
			into "$buildDir/dependency-libs"
			from configurations.hadoopruntime
		}
		task copyHadoopRuntimeDepsAll(type: Copy) {
			into "$buildDir/dependency-all-libs"
			from configurations.runtime
		}
		test.dependsOn([copyHadoopRuntimeDeps,copyHadoopRuntimeDepsAll])
	}

	project('spring-yarn:spring-yarn-test') {
		description = 'Spring Yarn Test'
		configurations {
			hadoopruntime.exclude group: 'log4j'
			hadoopruntime.exclude group: 'org.slf4j'
			hadoopruntime.exclude group: 'org.apache.hadoop'
			hadoopruntime.exclude group: 'commons-logging'
			hadoopruntime.exclude group: 'org.codehaus.jettison'
			hadoopruntime.exclude group: 'com.thoughtworks.xstream'
		}
		dependencies {
			compile project(":spring-yarn:spring-yarn-core")
			compile project(":spring-yarn:spring-yarn-test-core")
			hadoopruntime configurations.runtime
		}
		task copyHadoopRuntimeDeps(type: Copy) {
			into "$buildDir/dependency-libs"
			from configurations.hadoopruntime
		}
		task copyHadoopRuntimeDepsAll(type: Copy) {
			into "$buildDir/dependency-all-libs"
			from configurations.runtime
		}
		test.dependsOn([copyHadoopRuntimeDeps,copyHadoopRuntimeDepsAll])
	}

}

task wrapper(type: Wrapper) {
	description = "Generates gradlew[.bat] scripts"
	gradleVersion = "1.6"
}
	
