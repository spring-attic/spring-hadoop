<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" version="5.0"  xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xml:id="fs">

    <title>Working with the Hadoop File System</title>
    
    <para>A common task in Hadoop is interacting with its file system, whether for provisioning, adding new files to be processed, parsing results, or performing cleanup. Hadoop offers several ways to achieve that:
    one can use its Java API (namely <ulink url="http://hadoop.apache.org/common/docs/stable/api/index.html?org/apache/hadoop/fs/FileSystem.html"><literal>FileSystem</literal></ulink>) or 
    use the <literal>hadoop</literal> command line, in particular the file system <ulink url="http://hadoop.apache.org/common/docs/stable/file_system_shell.html">shell</ulink>. However there is no middle ground,
    one either has to use the (somewhat verbose, full of checked exceptions) API or fall back to the command line, outside the application. SHDP addresses this issue by bridging the two worlds, exposing both the 
    <literal>FileSystem</literal> and the fs shell through an intuitive, easy-to-use Java API. Add your favorite <ulink url="http://en.wikipedia.org/wiki/List_of_JVM_languages">JVM scripting</ulink> language right 
    inside your Spring for Apache Hadoop application and you have a powerful combination.</para>
    
 	<section id="fs:hdfs">
 		<title>Configuring the file-system</title>
 	
 		<para>The Hadoop file-system, HDFS, can be accessed in various ways - this section will cover the most popular protocols for interacting with HDFS and their pros and cons. SHDP does not enforce any specific protocol
 		to be used - in fact, as described in this section any <literal>FileSystem</literal> implementation can be used, allowing even other implementations than HDFS to be used.</para> 
 		
 		<para>The table below describes the common HDFS APIs in use:</para>
 		
 		<table id="fs:hdfs:api" pgwide="1" align="center">
 			<title>HDFS APIs</title>
 			
 			<tgroup cols="5">
 			<colspec colname="c1" colwidth="1*"/>
    		<colspec colname="c2" colwidth="1*"/>
    		<colspec colname="c3" colwidth="1*"/>
    		<colspec colname="c4" colwidth="1*"/>
    		<colspec colname="c5" colwidth="1*"/>
    		
    		<thead>
          	   <row>
          	     <entry>File System</entry>
          	     <entry>Comm. Method</entry>
          	     <entry>Scheme / Prefix</entry>
          	     <entry>Read / Write</entry>
          	     <entry>Cross Version</entry>
          	   </row>
       	    </thead>
       	    <tbody>
          	   <row>
          	      <entry>HDFS</entry>
          	      <entry>RPC</entry>
          	      <entry><literal>hdfs://</literal></entry>
          	      <entry>Read / Write</entry>
          	      <entry>Same HDFS version only</entry>
          	    </row>
          	   <row>
          	      <entry>HFTP</entry>
          	      <entry>HTTP</entry>
          	      <entry><literal>hftp://</literal></entry>
          	      <entry>Read only</entry>
          	      <entry>Version independent</entry>
          	    </row>
          	   <row>
          	      <entry>WebHDFS</entry>
          	      <entry>HTTP (REST)</entry>
          	      <entry><literal>webhdfs://</literal></entry>
          	      <entry>Read / Write</entry>
          	      <entry>Version independent</entry>
          	    </row>
          	</tbody>
          	</tgroup>   
 		</table>
 		
 		<sidebar>
 		  <title>What about FTP, Kosmos, S3 and the other file systems?</title>
 		  
 		  <para>This chapter focuses on the core file-system protocols supported by Hadoop. S3 (see the <link linkend="appendix-amazon-emr">Appendix</link>), FTP and the rest of the other <literal>FileSystem</literal>
 		   implementations are supported as well - Spring for Apache Hadoop has no dependency on the underlying system rather just on the public Hadoop API.</para>
 		</sidebar>
 		  
 		<para><literal>hdfs://</literal> protocol should be familiar to most readers - most docs (and in fact the previous chapter as well) mention it. It works out of the box and it's fairly efficient. However because it is
 		RPC based, it requires both the client and the Hadoop cluster to share the same version. Upgrading one without the other causes serialization errors meaning the client cannot interact with the cluster. As an alternative
 		one can use <literal>hftp://</literal> which is HTTP-based or its more secure brother <literal>hsftp://</literal> (based on SSL) which gives you a version independent protocol meaning you can use it to interact
 		with clusters with an unknown or different version than that of the client. <literal>hftp</literal> is read only (write operations will fail right away) and it is typically used with <literal>disctp</literal> for
 		reading data. <literal>webhdfs://</literal> is one of the additions in Hadoop 1.0 and is a mixture between <literal>hdfs</literal> and <literal>hftp</literal> protocol - it provides a version-independent, read-write, 
 		REST-based protocol which means that you can read and write to/from Hadoop clusters no matter their version. Furthermore, since <literal>webhdfs://</literal> is backed by a REST API, clients in other languages can
 		use it with minimal effort.</para>
 		
 		<note>
 			<para>Not all file systems work out of the box. For example WebHDFS needs to be enabled first in the cluster (through <literal>dfs.webhdfs.enabled</literal> property, see this 
 			<ulink url="http://hadoop.apache.org/common/docs/r1.0.0/webhdfs.html#Document+Conventions">document</ulink> for more information) while the secure <literal>hftp</literal>, <literal>hsftp</literal> 
 			requires the SSL configuration (such as certificates) to be specified. More about this (and how to use <literal>hftp/hsftp</literal> for proxying) in 
 			this <ulink url="http://hadoop.apache.org/hdfs/docs/r0.21.0/hdfsproxy.html">page</ulink>.</para> 
 		</note>
 		 		
 		<para>Once the scheme has been decided upon, one can specify it through the standard Hadoop <link linkend="hadoop:config">configuration</link>, either through the Hadoop configuration files or its properties:</para>
 		
 		<programlisting language="xml"><![CDATA[<hdp:configuration>
  fs.default.name=webhdfs://localhost
  ...
</hdp:configuration>]]></programlisting>
 		
 		<para>This instructs Hadoop (and automatically SHDP) what the default, implied file-system is. In SHDP, one can create additional file-systems (potentially to connect to other clusters) and specify a different 
 		scheme:</para>
 		
 		<programlisting language="xml"><![CDATA[<!-- manually creates the default SHDP file-system named 'hadoopFs' -->
<hdp:file-system uri="webhdfs://localhost"/>
 
<!-- creates a different FileSystem instance --> 
<hdp:file-system id="old-cluster" uri="hftp://old-cluster/"/>]]></programlisting>
 
 		<para>As with the rest of the components, the file systems can be injected where needed - such as file shell or inside scripts (see the next section).</para>
 		
 	</section>

    <section id="fs:hdfsrl">

      <title>Using HDFS Resource Loader</title>

      <para>In Spring the <interfacename>ResourceLoader</interfacename> interface is meant to be implemented
      by objects that can return (i.e. load) Resource instances.</para>

      <programlisting language="java"><![CDATA[public interface ResourceLoader {
  Resource getResource(String location);
}]]></programlisting>

      <para>All application contexts implement the <interfacename>ResourceLoader</interfacename> interface,
      and therefore all application contexts may be used to obtain Resource instances.</para>

      <para>When you call <literal>getResource()</literal> on a specific application context, and the location
      path specified doesn't have a specific prefix, you will get back a <literal>Resource</literal> type
      that is appropriate to that particular application context. For example, assume the following
      snippet of code was executed against a <classname>ClassPathXmlApplicationContext</classname> instance:</para>

      <programlisting language="java"><![CDATA[Resource template = ctx.getResource("some/resource/path/myTemplate.txt");]]></programlisting>

      <para>What would be returned would be a <literal>ClassPathResource</literal>; if the same method
      was executed against a <classname>FileSystemXmlApplicationContext</classname> instance, you'd get
      back a <classname>FileSystemResource</classname>. For a <classname>WebApplicationContext</classname>,
      you'd get back a <classname>ServletContextResource</classname>, and so on.</para>

      <para>As such, you can load resources in a fashion appropriate to the particular application context.</para>

      <para>On the other hand, you may also force <classname>ClassPathResource</classname> to be used,
      regardless of the application context type, by specifying the special <literal>classpath:</literal>
      prefix:</para>

      <programlisting language="java"><![CDATA[Resource template = ctx.getResource("classpath:some/resource/path/myTemplate.txt");]]></programlisting>

      <note><para>More information about the generic usage of resource loading, 
      check the <emphasis>Spring Framework Documentation</emphasis>.</para></note>

      <para><literal>Spring Hadoop</literal> is adding its own functionality into generic concept of
      resource loading. <classname>Resource</classname> abstraction in Spring has always been
      a way to ease resource access in terms of not having a need to know where there resource
      is and how it's accessed. This abstraction also goes beyond a single resource by allowing
      to use patterns to access multiple resources.</para>

      <para>Lets first see how <classname>HdfsResourceLoader</classname> is used manually.</para>

      <programlisting language="xml"><![CDATA[
<hdp:file-system />
<hdp:resource-loader id="loader" file-system-ref="hadoopFs" />
<hdp:resource-loader id="loaderWithUser" user="myuser" uri="hdfs://localhost:8020" />
]]></programlisting>

      <para>In above configuration we created two beans, one with reference to
      existing <literal>Hadoop FileSystem bean</literal> and one with
      impersonated user.</para>

      <programlisting language="java"><![CDATA[// get path '/tmp/file.txt'
Resource resource = loader.getResource("/tmp/file.txt");
// get path '/tmp/file.txt' with user impersonation
Resource resource = loaderWithUser.getResource("/tmp/file.txt");

// get path '/user/<current user>/file.txt'
Resource resource = loader.getResource("file.txt");
// get path '/user/myuser/file.txt'
Resource resource = loaderWithUser.getResource("file.txt");

// get all paths under '/tmp/'
Resource[] resources = loader.getResources("/tmp/*");
// get all paths under '/tmp/' recursively
Resource[] resources = loader.getResources("/tmp/**/*");
// get all paths under '/tmp/' using more complex ant path matching
Resource[] resources = loader.getResources("/tmp/?ile?.txt");]]></programlisting>

      <para>What would be returned in above examples would be instances of
      <classname>HdfsResource</classname>s. </para>

      <para>If there is a need for <emphasis>Spring Application Context</emphasis> to be
      aware of <classname>HdfsResourceLoader</classname> it needs to be registered
      using <literal>hdp:resource-loader-registrar</literal> namespace tag.</para>

      <programlisting language="xml"><![CDATA[
<hdp:file-system />
<hdp:resource-loader file-system-ref="hadoopFs" handle-noprefix="false" />
<hdp:resource-loader-registrar />
]]></programlisting>

      <note><para>On default the <classname>HdfsResourceLoader</classname> will
      handle all resource paths without prefix. Attribute <literal>handle-noprefix</literal>
      can be used to control this behaviour. If this attribute is set to
      <emphasis>false</emphasis>, non-prefixed resource uris will be handled by
      <emphasis>Spring Application Context</emphasis>.</para></note>

      <programlisting language="java"><![CDATA[// get 'default.txt' from current user's home directory
Resource[] resources = context.getResources("hdfs:default.txt");
// get all files from hdfs root
Resource[] resources = context.getResources("hdfs:/*");
// let context handle classpath prefix
Resource[] resources = context.getResources("classpath:cfg*properties");]]></programlisting>

      <para>What would be returned in above examples would be instances of
      <classname>HdfsResource</classname>s and <classname>ClassPathResource</classname>
      for the last one. If requesting resource paths without existing prefix,
      this example would fall back into <emphasis>Spring Application Context</emphasis>.
      It may be advisable to let <classname>HdfsResourceLoader</classname> to handle
      paths without prefix if your application doesn't rely on loading
      resources from underlying context without prefixes.</para>

      <table id="fs:hdfsrl:resourceloaderattributes" pgwide="1" align="center">
        <title><literal>hdp:resource-loader</literal> attributes</title>
        <tgroup cols="3">
          <colspec colname="c1" colwidth="1*"/>
          <colspec colname="c2" colwidth="1*"/>
          <colspec colname="c3" colwidth="4*"/>
          <spanspec spanname="description" namest="c2" nameend="c3" align="center"/>
          <thead>
            <row>
              <entry>Name</entry>
              <entry>Values</entry>
              <entry align="center">Description</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry><literal>file-system-ref</literal></entry>
              <entry>Bean Reference</entry>
              <entry>Reference to existing <emphasis>Hadoop FileSystem</emphasis> bean</entry>
            </row>
            <row>
              <entry><literal>use-codecs</literal></entry>
              <entry>Boolean(defaults to true)</entry>
              <entry>Indicates whether to use (or not) the codecs found inside the Hadoop configuration
              when accessing the resource input stream.</entry>
            </row>
            <row>
              <entry><literal>user</literal></entry>
              <entry>String</entry>
              <entry>The security user (ugi) to use for impersonation at runtime.</entry>
            </row>
            <row>
              <entry><literal>uri</literal></entry>
              <entry>String</entry>
              <entry>The underlying HDFS system URI.</entry>
            </row>
            <row>
              <entry><literal>handle-noprefix</literal></entry>
              <entry>Boolean(defaults to true)</entry>
              <entry>Indicates if loader should handle resource paths without prefix.</entry>
            </row>
          </tbody>
        </tgroup>  
      </table>

      <table id="fs:hdfsrl:resourceloaderregistrarattributes" pgwide="1" align="center">
        <title><literal>hdp:resource-loader-registrar</literal> attributes</title>
        <tgroup cols="3">
          <colspec colname="c1" colwidth="1*"/>
          <colspec colname="c2" colwidth="1*"/>
          <colspec colname="c3" colwidth="4*"/>
          <spanspec spanname="description" namest="c2" nameend="c3" align="center"/>
          <thead>
            <row>
              <entry>Name</entry>
              <entry>Values</entry>
              <entry align="center">Description</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry><literal>loader-ref</literal></entry>
              <entry>Bean Reference</entry>
              <entry>Reference to existing <emphasis>Hdfs resource loader</emphasis> bean.
              Default value is 'hadoopResourceLoader'.</entry>
            </row>
          </tbody>
        </tgroup>  
      </table>

    </section>
    
    <section id="scripting:api" xmlns="http://docbook.org/ns/docbook" version="5.0"  xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xml:id="scripting">
    	<title>Scripting the Hadoop API</title>
    	
    	<sidebar>
    	  <title>Supported scripting languages</title>
    	  
    	  <para>SHDP scripting supports any <ulink url="http://www.jcp.org/en/jsr/detail?id=223">JSR-223</ulink> (also known as <literal>javax.scripting</literal>) compliant scripting engine. Simply add the 
    	  engine jar to the classpath and the application should be able to find it. Most languages (such as Groovy or JRuby) provide JSR-233 support out of the box; for those that do not see the 
    	  <ulink url="http://java.net/projects/scripting">scripting</ulink> project that provides various adapters.</para>
    	</sidebar>
    
    	<para>Since Hadoop is written in Java, accessing its APIs in a <emphasis>native</emphasis> way provides maximum control and flexibility over the interaction with Hadoop. This holds true for working with
    	its file systems; in fact all the other tools that one might use are built upon these. The main entry point is the <classname>org.apache.hadoop.fs.FileSystem</classname> abstract class which provides the
    	foundation of most (if not all) of the actual file system implementations out there. Whether one is using a local, remote or distributed store through the <classname>FileSystem</classname> API she
    	can query and manipulate the available resources or create new ones. To do so however, one needs to write Java code, compile the classes and configure them which is somewhat cumbersome especially when
    	performing simple, straightforward operations (like copy a file or delete a directory).</para>
    	
    	<para>JVM scripting languages (such as <ulink url="http://groovy.codehaus.org/">Groovy</ulink>, <ulink url="http://jruby.org/">JRuby</ulink>, <ulink url="http://www.jython.org/">Jython</ulink> or 
    	<ulink url="http://www.mozilla.org/rhino/">Rhino</ulink> to name just a few) provide a nice solution to the Java language; they run on the JVM, can interact with the Java code with no or few
    	changes or restrictions and have a nicer, simpler, less <emphasis>ceremonial</emphasis> syntax; that is, there is no need to define a class or a method - simply write the code that you want to execute and you are done.
    	SHDP combines the two, taking care of the configuration and the infrastructure so one can interact with the Hadoop environment from her language of choice.</para>
    	
    	<para>Let us take a look at a JavaScript example using Rhino (which is part of JDK 6 or higher, meaning one does not need any extra libraries):</para>

		<programlisting language="xml"><![CDATA[<beans xmlns="http://www.springframework.org/schema/beans" ...>		
  <hdp:configuration .../>
		
  <hdp:script id="inlined-js" language="javascript" run-at-startup="true">
    try {load("nashorn:mozilla_compat.js");} catch (e) {} // for Java 8
    importPackage(java.util);

    name = UUID.randomUUID().toString()
    scriptName = "src/test/resources/test.properties"
    ]]>// <emphasis role="strong">fs</emphasis> - FileSystem instance based on 'hadoopConfiguration' bean
    // call FileSystem#copyFromLocal(Path, Path)  
    <emphasis role="strong">fs</emphasis>.copyFromLocalFile(scriptName, name)
    // return the file length 
    <emphasis role="strong">fs</emphasis>.getLength(name)<![CDATA[
  </hdp:script>
	 
</beans>]]></programlisting>

		<para>The <literal>script</literal> element, part of the SHDP namespace, builds on top of the scripting support in Spring permitting script declarations to be evaluated and declared as normal bean definitions. Furthermore it automatically exposes Hadoop-specific objects, based on the existing configuration, to the script such as the <literal>FileSystem</literal> (more on that in the next section). As one can see, the script
		is fairly obvious: it generates a random name (using the <classname>UUID</classname> class from <literal>java.util</literal> package) and then copies a local file into HDFS under the random name. The last line returns
		the length of the copied file which becomes the value of the declaring bean (in this case <literal>inlined-js</literal>) - note that this might vary based on the scripting engine used.</para>
		
		<note>The attentive reader might have noticed that the arguments passed to the <literal>FileSystem</literal> object are not of type <literal>Path</literal> but rather <literal>String</literal>. To avoid
		the creation of <literal>Path</literal> object, SHDP uses a wrapper class (<literal>SimplerFileSystem</literal>) which automatically does the conversion so you don't have to. For more information see the 
		<link linkend="scripting:vars">implicit variables</link> section.</note>
		<para>Note that for inlined scripts, one can use Spring's property placeholder configurer to automatically expand variables at runtime. Using one of the examples seen before:</para>
		
		<programlisting language="xml"><![CDATA[<beans ... >
  <context:property-placeholder location="classpath:hadoop.properties" />
   
  <hdp:script language="javascript" run-at-startup="true">
    ...
    tracker=]]><emphasis role="strong">${hd.fs}</emphasis><![CDATA[
    ...
  </hdp:script>
</beans>]]></programlisting>

		<para>Notice how the script above relies on the property placeholder to expand <literal>${hd.fs}</literal> with the values from <literal>hadoop.properties</literal> file available in the classpath.</para>

	    <para>As you might have noticed, the <literal>script</literal> element defines a runner for JVM scripts. And just like the rest of the SHDP runners, it allows one or multiple 
	    <literal>pre</literal> and <literal>post</literal> actions to be specified to be executed before and after each run. 
		Typically other runners (such as other jobs or scripts) can be specified but any JDK <literal>Callable</literal> can be 
		passed in. Do note that the runner will not run unless triggered manually or if <literal>run-at-startup</literal> is set to <literal>true</literal>.  
		For more information on runners, see the <link linkend="runners">dedicated</link> chapter.</para>
		
		<para></para>
		<section id="scripting:api:scripts">
			<title>Using scripts</title>
			
			<para>Inlined scripting is quite handy for doing simple operations and coupled with the property expansion is quite a powerful tool that can handle a variety of use cases. However when more logic is required
or the script is affected by XML formatting, encoding or syntax restrictions (such as Jython/Python for which white-spaces are important) one should consider externalization. That is, rather than declaring the script
directly inside the XML, one can declare it in its own file. And speaking of Python, consider the variation of the previous example:</para>

			<programlisting language="xml"><![CDATA[<hdp:script location="org/company/basic-script.py" run-at-startup="true"/>]]></programlisting>
			
			<para>The definition does not bring any surprises but do notice there is no need to specify the language (as in the case of a inlined declaration) 
			since script extension (<literal>py</literal>) already provides that information. Just for completeness, the <literal>basic-script.py</literal> looks as follows:</para>
			
			<programlisting language="python"><![CDATA[from java.util import UUID
from org.apache.hadoop.fs import Path

print "Home dir is " + str(fs.homeDirectory)
print "Work dir is " + str(fs.workingDirectory)
print "/user exists " + str(fs.exists("/user"))

name = UUID.randomUUID().toString()
scriptName = "src/test/resources/test.properties"
fs.copyFromLocalFile(scriptName, name)
print Path(name).makeQualified(fs)]]></programlisting>

		</section>
	</section>
	
	<section id="scripting:vars">
		<title>Scripting implicit variables</title>
		
		<para>To ease the interaction of the script with its enclosing context, SHDP binds by default the so-called <emphasis>implicit</emphasis> variables. These are:</para>
		<table id="scripting:vars:tbl" pgwide="1" align="center">
			<title>Implicit variables</title>
		
			<tgroup cols="3">
    		 <colspec colname="c1" colwidth="1*"/>
    		 <colspec colname="c2" colwidth="1*"/>
    		 <colspec colname="c3" colwidth="4*"/>
          	 <spanspec spanname="description" namest="c2" nameend="c3" align="center"/>
          		
          	  <thead>
          	   <row>
          	     <entry>Name</entry>
          	     <entry>Type</entry>
          	     <entry align="center">Description</entry>
          	   </row>
          	  </thead>
          	  
          	  <tbody>
          	    <row>
          	      <entry>cfg</entry>
          	      <entry><literal>org.apache.hadoop.conf.Configuration</literal></entry>
          	      <entry>Hadoop Configuration (relies on <emphasis>hadoopConfiguration</emphasis> bean or singleton type match)</entry>
          	    </row>
          	    <row>
          	      <entry>cl</entry>
          	      <entry><literal>java.lang.ClassLoader</literal></entry>
          	      <entry>ClassLoader used for executing the script</entry>
          	    </row>
          	    <row>
          	      <entry>ctx</entry>
          	      <entry><literal>org.springframework.context.ApplicationContext</literal></entry>
          	      <entry>Enclosing application context</entry>
          	    </row>
          	    <row>
          	      <entry>ctxRL</entry>
          	      <entry><literal>org.springframework.io.support.ResourcePatternResolver</literal></entry>
          	      <entry>Enclosing application context ResourceLoader</entry>
          	    </row>
          	    <row>
          	      <entry>distcp</entry>
          	      <entry><literal>org.springframework.data.hadoop.fs.DistributedCopyUtil</literal></entry>
          	      <entry>Programmatic access to DistCp</entry>
          	    </row>
          	    <row>
          	      <entry>fs</entry>
          	      <entry><literal>org.apache.hadoop.fs.FileSystem</literal></entry>
          	      <entry>Hadoop File System (relies on 'hadoop-fs' bean or singleton type match, falls back to creating one based on 'cfg')</entry>
          	    </row>
          	    <row>
          	      <entry>fsh</entry>
          	      <entry><literal>org.springframework.data.hadoop.fs.FsShell</literal></entry>
          	      <entry>File System shell, exposing hadoop 'fs' commands as an API</entry>
          	    </row>
          	    <row>
          	      <entry>hdfsRL</entry>
          	      <entry><literal>org.springframework.data.hadoop.io.HdfsResourceLoader</literal></entry>
          	      <entry>Hdfs resource loader (relies on 'hadoop-resource-loader' or singleton type match, falls back to creating one automatically based on 'cfg')</entry>
          	    </row>
          	  </tbody>
          	</tgroup>  
		</table>
		
		<note>If no Hadoop <literal>Configuration</literal> can be detected (either by name <literal>hadoopConfiguration</literal> or by type), several log warnings will be made and none of the Hadoop-based variables (namely 
		<literal>cfg</literal>, <literal>distcp</literal>, <literal>fs</literal>, <literal>fsh</literal>, <literal>distcp</literal> or <literal>hdfsRL</literal>) will be bound.
		</note>
		<para>As mentioned in the <emphasis>Description</emphasis> column, the variables are first looked (either by name or by type) in the application context and, in case they are missing, created on the spot based on
		the existing configuration. Note that it is possible to override or add new variables to the scripts through the <literal>property</literal> sub-element that can set values or references to other beans:</para>
		
		<programlisting language="xml"><![CDATA[<hdp:script location="org/company/basic-script.js" run-at-startup="true">
   <hdp:property name="foo" value="bar"/>
   <hdp:property name="ref" ref="some-bean"/>
</hdp:script>]]></programlisting>

	<section id="scripting:lifecycle">
		<title>Running scripts</title>
		
		<para>The <literal>script</literal> namespace provides various options to adjust its behaviour depending on the script content. By default the script is simply declared - that is, no execution occurs. 
		One however can change that so that the script gets evaluated at startup (as all the examples in this section do) through the <literal>run-at-startup</literal> flag 
		(which is by default <literal>false</literal>) or when invoked manually
		(through the <interfacename>Callable</interfacename>).
		Similarily, by default the script gets evaluated on each run. However for scripts that are expensive and return the same
		value every time one has various <emphasis>caching</emphasis> options, so the evaluation occurs only when needed through the <literal>evaluate</literal> attribute:</para>
		
		<table id="scripting:lifecycle:flags" pgwide="1" align="center">
			<title><literal>script</literal> attributes</title>
		
			<tgroup cols="3">
    		 <colspec colname="c1" colwidth="1*"/>
    		 <colspec colname="c2" colwidth="1*"/>
    		 <colspec colname="c3" colwidth="4*"/>
          	 <spanspec spanname="description" namest="c2" nameend="c3" align="center"/>
          		
          	  <thead>
          	   <row>
          	     <entry>Name</entry>
          	     <entry>Values</entry>
          	     <entry align="center">Description</entry>
          	   </row>
          	  </thead>
          	  
          	  <tbody>
          	    <row>
          	      <entry><literal>run-at-startup</literal></entry>
          	      <entry><literal>false</literal>(default), <literal>true</literal></entry>
          	      <entry>Wether the script is executed at startup or not</entry>
          	    </row>
          	    <row>
          	      <entry><literal>evaluate</literal></entry>
          	      <entry><literal>ALWAYS</literal>(default), <literal>IF_MODIFIED</literal>, <literal>ONCE</literal></entry>
          	      <entry>Wether to actually evaluate the script when invoked or used a previous value. <literal>ALWAYS</literal> means evaluate every time, <literal>IF_MODIFIED</literal> evaluate if the backing
          	      resource (such as a file) has been modified in the meantime and <literal>ONCE</literal> only once.</entry>
          	    </row>
          	  </tbody>
          	</tgroup>  
		</table>
	</section>

	<section id="scripting-tasklet">
			<title>Using the Scripting tasklet</title>
	
			<para>For Spring Batch environments, SHDP provides a dedicated tasklet to execute scripts.</para>
			
			<programlisting language="xml"><![CDATA[<script-tasklet id="script-tasklet">
  <script language="groovy">
    inputPath = "/user/gutenberg/input/word/"
    outputPath = "/user/gutenberg/output/word/"
    if (fsh.test(inputPath)) {
      fsh.rmr(inputPath)
    }
    if (fsh.test(outputPath)) {
      fsh.rmr(outputPath)
    }
    inputFile = "src/main/resources/data/nietzsche-chapter-1.txt"
    fsh.put(inputFile, inputPath)
  </script>
</script-tasklet>]]></programlisting>

			<para>The tasklet above embedds the script as a nested element.  You can also declare a reference to another script definition, using the script-ref attribute which allows you to externalize the scripting code to an external resource.</para>	
			<programlisting language="xml"><![CDATA[<script-tasklet id="script-tasklet" script-ref="clean-up"/>
	<hdp:script id="clean-up" location="org/company/myapp/clean-up-wordcount.groovy"/>]]></programlisting>
		</section>   
    </section>
    
    <section id="scripting-fssh">
    	<title>File System Shell (FsShell)</title>
    	
    	<para>A handy utility provided by the Hadoop distribution is the file system <ulink url="http://hadoop.apache.org/common/docs/stable/file_system_shell.html">shell</ulink> which allows UNIX-like commands to be
    	executed against HDFS. One can check for the existence of files, delete, move, copy directories or files or set up permissions. However the utility is only available from the command-line which makes it hard
    	to use from/inside a Java application. To address this problem, SHDP provides a lightweight, fully embeddable shell, called <literal>FsShell</literal> which mimics most of the commands available from the command line:
    	rather than dealing with <literal>System.in</literal> or <literal>System.out</literal>, one deals with objects.</para>
    	
    	<para>Let us take a look at using <literal>FsShell</literal> by building on the previous scripting examples:</para>
    	
    	<programlisting language="xml"><![CDATA[<hdp:script location="org/company/basic-script.groovy" run-at-startup="true"/>]]></programlisting>
    	
		<programlisting language="java"><![CDATA[name = UUID.randomUUID().toString()
scriptName = "src/test/resources/test.properties"
fs.copyFromLocalFile(scriptName, name)

// use the shell made available under variable ]]><emphasis role="strong">fsh</emphasis>
dir = "script-dir"
if (!fsh.test(dir)) {
   fsh.mkdir(dir); fsh.cp(name, dir); fsh.chmodr(700, dir)
   println "File content is " + fsh.cat(dir + name).toString()
}
println fsh.ls(dir).toString()
fsh.rmr(dir)
</programlisting>

		<para>As mentioned in the previous section, a <literal>FsShell</literal> instance is automatically created and configured for scripts, under the name <emphasis>fsh</emphasis>. 
		Notice how the entire block relies on the usual	commands: <literal>test</literal>, <literal>mkdir</literal>, <literal>cp</literal> and so on. 
		Their semantics are exactly the same as in the command-line version however one has access to a native Java API that returns
		actual objects (rather than <literal>String</literal>s) making it easy to use them programmatically whether in Java or another language. Furthermore, the class offers enhanced methods (such as <literal>chmodr</literal>
		which stands for <emphasis>recursive</emphasis> <literal>chmod</literal>) and multiple overloaded methods taking advantage of <ulink url="http://docs.oracle.com/javase/1.5.0/docs/guide/language/varargs.html">varargs</ulink> 
		so that multiple parameters can be specified. Consult the <ulink url="http://static.springsource.org/spring-hadoop/docs/current/api/index.html?org/springframework/data/hadoop/fs/FsShell.html">API</ulink> for more information.
		</para>
		
		<para>To be as close as possible to the command-line shell, <literal>FsShell</literal> mimics even the messages being displayed. Take a look at line 9 which prints the result of <literal>fsh.cat()</literal>. The
		method returns a <literal>Collection</literal> of Hadoop <literal>Path</literal> objects (which one can use programatically). However when invoking <literal>toString</literal> on the collection, the same printout as from
		the command-line shell is being displayed:</para>
		
		<programlisting><![CDATA[File content is ]]><emphasis>some text</emphasis></programlisting>
		
		<para>The same goes for the rest of the methods, such as <literal>ls</literal>. The same script in JRuby would look something like this:</para>

		<programlisting language="ruby"><![CDATA[require 'java'
name = java.util.UUID.randomUUID().to_s
scriptName = "src/test/resources/test.properties"
$fs.copyFromLocalFile(scriptName, name)

# use the shell
dir = "script-dir/"
...
print $fsh.ls(dir).to_s]]></programlisting>

		<para>which prints out something like this:</para>
		
		<programlisting><![CDATA[drwx------   - user     supergroup          0 2012-01-26 14:08 /user/user/script-dir
-rw-r--r--   3 user     supergroup        344 2012-01-26 14:08 /user/user/script-dir/520cf2f6-a0b6-427e-a232-2d5426c2bc4e]]></programlisting>

		<para>As you can see, not only can you reuse the existing tools and commands with Hadoop inside SHDP, but you can also code against them in various scripting languages. And as you might have noticed, there is no
		special configuration required - this is automatically inferred from the enclosing application context.</para>
		
		<note>The careful reader might have noticed that besides the syntax, there are some minor differences in how the various languages interact with the java objects. For example the automatic <literal>toString</literal>
		call called in Java for doing automatic <literal>String</literal> conversion is not necessarily supported (hence the <literal>to_s</literal> in Ruby or <literal>str</literal> in Python). This is to be expected
		as each language has its own semantics - for the most part these are easy to pick up but do pay attention to details.</note>


		<section id="scripting:fssh:distcp">
			<title>DistCp API</title>
			
			<para>Similar to the <literal>FsShell</literal>, SHDP provides a lightweight, fully embeddable <ulink url="http://hadoop.apache.org/common/docs/stable/distcp.html"><literal>DistCp</literal></ulink> version 
			that builds on top of the <literal>distcp</literal> from the Hadoop distro.	The semantics and configuration options are the same however, one can use it from within a Java application without having to use 
			the command-line. See the <ulink url="http://static.springsource.org/spring-hadoop/docs/current/api/index.html?org/springframework/data/hadoop/fs/DistCp.html">API</ulink> for more information:</para>
			
			<programlisting language="xml"><![CDATA[<hdp:script language="groovy">distcp.copy("${distcp.src}", "${distcp.dst}")</hdp:script>]]></programlisting>
			
			<para>The bean above triggers a distributed copy relying again on Spring's property placeholder variable expansion for its source and destination.</para>
		</section>
    </section>
</chapter>
