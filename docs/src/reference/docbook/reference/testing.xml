<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" version="5.0" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xml:id="testing">

  <title>Testing Support</title>

  <para>Hadoop testing has always been a cumbersome process especially if you
  try to do testing phase during the normal project build process.
  Traditionally developers have had few options like running Hadoop cluster
  either as a local or pseudo-distributed mode and then utilise that to
  run MapReduce jobs. Hadoop project itself is using a lot of mini clusters
  during the tests which provides better tools to run your code in an isolated
  environment.</para>

  <para>Spring Hadoop and especially its Yarn module faced similar testing problems.
  Spring Hadoop provides testing facilities order to make testing on Hadoop much easier
  especially if code relies on Spring Hadoop itself. These testing facilities are also used
  internally to test Spring Hadoop, although some test cases still rely on a running Hadoop
  instance on a host where project build is executed.</para>

  <para>Two central concepts of testing using Spring Hadoop is, firstly fire up the mini
  cluster and secondly use the configuration prepared by the mini cluster to talk to the
  Hadoop components. Now let's go through the general testing facilities offered by
  Spring Hadoop.</para>

  <para>Testing for MapReduce and Yarn in Spring Hadoop is separated into different
  packages mostly because these two components doesn't have hard dependencies
  with each others. You will see a lot of similarities when creating tests
  for MapReduce and Yarn.</para>

  <section id="testing:mr">

    <title>Testing MapReduce</title>

    <section id="testing:mr:minicluster">

      <title>Mini Clusters for MapReduce</title>

      <para>Mini clusters usually contain testing components from a Hadoop project
      itself. These are clusters for <emphasis>MapReduce Job</emphasis> handling and
      <emphasis>HDFS</emphasis> which are all run within a same process.
      In Spring Hadoop mini clusters are implementing interface
      <interfacename>HadoopCluster</interfacename> which provides methods for lifecycle
      and configuration. <emphasis>Spring Hadoop</emphasis> provides transitive
      maven dependencies against different <emphasis>Hadoop</emphasis> distributions
      and thus mini clusters are started using different implementations. This is mostly
      because we want to support <emphasis>HadoopV1</emphasis> and
      <emphasis>HadoopV2</emphasis> at a same time. All this is handled
      automatically at runtime so everything should be transparent to
      the end user.</para>

      <programlisting language="java"><![CDATA[public interface HadoopCluster {
  Configuration getConfiguration();
  void start() throws Exception;
  void stop();
  FileSystem getFileSystem() throws IOException;
}]]></programlisting>

      <para>Currently one implementation named <emphasis>StandaloneHadoopCluster</emphasis>
      exists which supports simple cluster type where a number of nodes can be defined
      and then all the nodes will contain utilities for <emphasis>MapReduce Job</emphasis> handling
      and <emphasis>HDFS</emphasis>.</para>

      <para>There are few ways how this cluster can be started depending on a use case. It is
      possible to use <classname>StandaloneHadoopCluster</classname> directly or configure and
      start it through <classname>HadoopClusterFactoryBean</classname>. Existing
      <classname>HadoopClusterManager</classname> is used in unit tests to
      cache running clusters.</para>

      <note><para>It's advisable not to use <classname>HadoopClusterManager</classname>
      outside of tests because literally it is using static fields to cache cluster references.
      This is a same concept used in <emphasis>Spring Test</emphasis> order to cache
      application contexts between the unit tests within a jvm.</para></note>

      <programlisting language="xml"><![CDATA[<bean id="hadoopCluster" class="org.springframework.data.hadoop.test.support.HadoopClusterFactoryBean">
  <property name="clusterId" value="HadoopClusterTests"/>
  <property name="autoStart" value="true"/>
  <property name="nodes" value="1"/>
</bean>]]></programlisting>

      <para>Example above defines a bean named <emphasis>hadoopCluster</emphasis> using a factory bean
      <classname>HadoopClusterFactoryBean</classname>. It defines a simple one node cluster which is
      started automatically.</para>

    </section>

    <section id="testing:mr:configuration">

      <title>Configuration</title>

      <para><emphasis>Spring Hadoop</emphasis> components usually depend on
      <emphasis>Hadoop</emphasis> configuration which is then wired into these components
      during the application context startup phase. This was explained
      in previous chapters so we don't go through it again. However this is now a catch-22 because
      we need the configuration for the context but it is not known until mini cluster has done
      its startup magic and prepared the configuration with correct values reflecting current
      runtime status of the cluster itself. Solution for this is to use other bean named
      <classname>ConfigurationDelegatingFactoryBean</classname> which will simply delegate
      the configuration request into the running cluster.</para>

      <programlisting language="xml"><![CDATA[<bean id="hadoopConfiguredConfiguration" class="org.springframework.data.hadoop.test.support.ConfigurationDelegatingFactoryBean">
  <property name="cluster" ref="hadoopCluster"/>
</bean>

<hdp:configuration id="hadoopConfiguration" configuration-ref="hadoopConfiguredConfiguration"/>]]></programlisting>

      <para>In the above example we created a bean named <emphasis>hadoopConfiguredConfiguration</emphasis>
      using <classname>ConfigurationDelegatingFactoryBean</classname> which simple delegates
      to <emphasis>hadoopCluster</emphasis> bean. Returned bean <emphasis>hadoopConfiguredConfiguration</emphasis>
      is type of <emphasis>Hadoop's</emphasis> <classname>Configuration</classname> object so it could
      be used as it is.</para>

      <para>Latter part of the example show how <emphasis>Spring Hadoop</emphasis> namespace is
      used to create another <classname>Configuration</classname> object which is using
      <emphasis>hadoopConfiguredConfiguration</emphasis> as a reference. This scenario
      would make sense if there is a need to add additional configuration options into running
      configuration used by other components. Usually it is suiteable to use cluster prepared
      configuration as it is.</para>

    </section>

    <section id="testing:mr:simplified">

      <title>Simplified Testing</title>

      <para>It is perfecly all right to create your tests from scratch and for example create
      the cluster manually and then get the runtime configuration from there. This just needs
      some boilerplate code in your context configuration and unit test lifecycle.</para>

      <para><emphasis>Spring Hadoop</emphasis> adds additional facilities for the testing
      to make all this even easier. </para>

      <programlisting language="java"><![CDATA[@RunWith(SpringJUnit4ClassRunner.class)
public abstract class AbstractHadoopClusterTests implements ApplicationContextAware {
  ...
}

@ContextConfiguration(loader=HadoopDelegatingSmartContextLoader.class)
@MiniHadoopCluster
public class ClusterBaseTestClassTests extends AbstractHadoopClusterTests {
  ...
}]]></programlisting>

      <para>Above example shows the <classname>AbstractHadoopClusterTests</classname> and
      how <classname>ClusterBaseTestClassTests</classname> is prepared to be aware of a mini cluster.
      <classname>HadoopDelegatingSmartContextLoader</classname> offers same base
      functionality as the default <classname>DelegatingSmartContextLoader</classname> in a
      spring-test package. One additional thing what <classname>HadoopDelegatingSmartContextLoader</classname>
      does is to automatically handle running clusters and inject <classname>Configuration</classname>
     into the application context.</para>

      <programlisting language="java"><![CDATA[@MiniHadoopCluster(configName="hadoopConfiguration", clusterName="hadoopCluster", nodes=1, id="default")]]></programlisting>

      <para>Generally <classname>@MiniHadoopCluster</classname> annotation allows you to define
      injected bean name for mini cluster, its Configurations and a number of nodes you
      like to have in a cluster.</para>

      <para><emphasis>Spring Hadoop</emphasis> testing is dependant of general facilities of
      <emphasis>Spring Test framework</emphasis> meaning that everything what is cached during
      the test are reuseable withing other tests. One need to understand that if <emphasis>Hadoop</emphasis>
      mini cluster and its <classname>Configuration</classname> is injected into an Application
      Context, caching happens on a mercy of a Spring Testing meaning if a test Application Context is
      cached also mini cluster instance is cached. While caching is always prefered, one needs to
      understant that if tests are expecting vanilla environment to be present, test context should be
      dirtied using <classname>@DirtiesContext</classname> annotation.</para>

    </section>

    <section id="testing:mr:wordcount">

      <title>Wordcount Example</title>

      <para>Let's study a proper example of existing <emphasis>MapReduce Job</emphasis> which
      is executed and tested using <emphasis>Spring Hadoop</emphasis>. This example is the
      Hadoop's classic wordcount. We don't go through all the details of this example because
      we want to concentrate on testing specific code and configuration.</para>

      <programlisting language="xml"><![CDATA[<context:property-placeholder location="hadoop.properties" />

<hdp:job id="wordcountJob"
  input-path="${wordcount.input.path}"
  output-path="${wordcount.output.path}"
  libs="file:build/libs/mapreduce-examples-wordcount-*.jar"
  mapper="org.springframework.data.hadoop.examples.TokenizerMapper"
  reducer="org.springframework.data.hadoop.examples.IntSumReducer" />

<hdp:script id="setupScript" location="copy-files.groovy">
  <hdp:property name="localSourceFile" value="data/nietzsche-chapter-1.txt" />
  <hdp:property name="inputDir" value="${wordcount.input.path}" />
  <hdp:property name="outputDir" value="${wordcount.output.path}" />
</hdp:script>

<hdp:job-runner id="runner"
  run-at-startup="false"
  kill-job-at-shutdown="false"
  wait-for-completion="false"
  pre-action="setupScript"
  job-ref="wordcountJob" />]]></programlisting>

      <para>In above configuration example we can see few differences with the actual
      runtime configuration. Firstly you can see that we didn't specify any kind
      of configuration for hadoop. This is because it's is injected automatically
      by testing framework. Secondly because we want to explicitely wait the job
      to be run and finished, <emphasis>kill-job-at-shutdown</emphasis> and
      <emphasis>wait-for-completion</emphasis> are set to <emphasis>false</emphasis>.</para>

      <programlisting language="java"><![CDATA[@ContextConfiguration(loader=HadoopDelegatingSmartContextLoader.class)
@MiniHadoopCluster
public class WordcountTests extends AbstractMapReduceTests {
  @Test
  public void testWordcountJob() throws Exception {
    // run blocks and throws exception if job failed
    JobRunner runner = getApplicationContext().getBean("runner", JobRunner.class);
    Job wordcountJob = getApplicationContext().getBean("wordcountJob", Job.class);

    runner.call();

    JobStatus finishedStatus = waitFinishedStatus(wordcountJob, 60, TimeUnit.SECONDS);
    assertThat(finishedStatus, notNullValue());

    // get output files from a job
    Path[] outputFiles = getOutputFilePaths("/user/gutenberg/output/word/");
    assertEquals(1, outputFiles.length);
    assertThat(getFileSystem().getFileStatus(outputFiles[0]).getLen(), greaterThan(0l));

    // read through the file and check that line with
    // "themselves	6" was found
    boolean found = false;
    InputStream in = getFileSystem().open(outputFiles[0]);
    BufferedReader reader = new BufferedReader(new InputStreamReader(in));
    String line = null;
    while ((line = reader.readLine()) != null) {
      if (line.startsWith("themselves")) {
        assertThat(line, is("themselves\t6"));
        found = true;
      }
    }
    reader.close();
    assertThat("Keyword 'themselves' not found", found);
  }
}]]></programlisting>

      <para>In above unit test class we simply run the job defined in xml,
      explicitely wait it to finish and then check the output content from
      <emphasis>HDFS</emphasis> by searching expected strings.</para>

    </section>

  </section>

  <section id="testing:yarn">

    <title>Testing Yarn</title>

    <section id="testing:yarn:minicluster">
      <title>Mini Clusters for Yarn</title>

      <para>Mini cluster usually contain testing components from a <emphasis>Hadoop</emphasis> project itself.
      These are <classname>MiniYARNCluster</classname> for Resource Manager and
      <classname>MiniDFSCluster</classname> for Datanode and Namenode
      which are all run within a same process. In <emphasis>Spring Hadoop</emphasis> mini
      clusters are implementing interface <interfacename>YarnCluster</interfacename> which provides
      methods for lifecycle and configuration.</para>

      <programlisting language="java"><![CDATA[public interface YarnCluster {
  Configuration getConfiguration();
  void start() throws Exception;
  void stop();
  File getYarnWorkDir();
}]]></programlisting>

      <para>Currently one implementation named <classname>StandaloneYarnCluster</classname> exists which supports simple
      cluster type where a number of nodes can be defined and then all the nodes will have <emphasis>Yarn Node
      Manager</emphasis> and <emphasis>Hdfs Datanode</emphasis>,  additionally
      a <emphasis>Yarn Resource Manager</emphasis> and <emphasis>Hdfs Namenode</emphasis>
      components are started.</para>

      <para>There are few ways how this cluster can be started depending on a use case. It is
      possible to use <classname>StandaloneYarnCluster</classname> directly or configure and start it through
      <classname>YarnClusterFactoryBean</classname>. Existing <classname>YarnClusterManager</classname>
      is used in unit tests to cache running clusters.</para>

      <note><para>It's advisable not to use <classname>YarnClusterManager</classname> outside of tests
      because literally it is using static fields to cache cluster references. This is a
      same concept used in <emphasis>Spring Test</emphasis> order to cache application contexts between
      the unit tests within a jvm.</para></note>

      <programlisting language="xml"><![CDATA[<bean id="yarnCluster" class="org.springframework.yarn.test.support.YarnClusterFactoryBean">
  <property name="clusterId" value="YarnClusterTests"/>
  <property name="autoStart" value="true"/>
  <property name="nodes" value="1"/>
</bean>]]></programlisting>

      <para>Example above defines a bean named <emphasis>yarnCluster</emphasis> using a factory
      bean <classname>YarnClusterFactoryBean</classname>. It defines a simple one node cluster
      which is started automatically. Cluster working directories
      would then exist under below paths:</para>

      <programlisting language="xml"><![CDATA[target/YarnClusterTests/
target/YarnClusterTests-dfs/]]></programlisting>

      <note><para>We rely on base classes from a <emphasis>Hadoop</emphasis> distribution and
      target base directory is hardcoded in Hadoop and is not configurable.</para></note>

    </section>

    <section id="testing:yarn:configuration">
      <title>Configuration</title>

      <para><emphasis>Spring Yarn</emphasis> components usually depend on <emphasis>Hadoop</emphasis>
      configuration which is then wired into these components during the application context startup
      phase. This was explained in previous chapters so we don't go through it again. However this
      is now a catch-22 because we need the configuration for the context but it is not known until
      mini cluster has done its startup magic and prepared the configuration with correct values reflecting current
      runtime status of the cluster itself. Solution for this is to use other factory bean class named
      <classname>ConfigurationDelegatingFactoryBean</classname> which will simple delegate the configuration request
      into the running cluster.</para>

      <programlisting language="xml"><![CDATA[<bean id="yarnConfiguredConfiguration" class="org.springframework.yarn.test.support.ConfigurationDelegatingFactoryBean">
  <property name="cluster" ref="yarnCluster"/>
</bean>

<yarn:configuration id="yarnConfiguration" configuration-ref="yarnConfiguredConfiguration"/>]]></programlisting>

      <para>In the above example we created a bean named <emphasis>yarnConfiguredConfiguration</emphasis>
      using <classname>ConfigurationDelegatingFactoryBean</classname> which simple delegates to
      <emphasis>yarnCluster</emphasis> bean. Returned bean <emphasis>yarnConfiguredConfiguration</emphasis>
      is type of <emphasis>Hadoop's</emphasis> <classname>Configuration</classname> object so it could
      be used as it is.</para>

      <para>Latter part of the example show how <emphasis>Spring Yarn</emphasis> namespace is used to
      create another <classname>Configuration</classname> object which is using
      <emphasis>yarnConfiguredConfiguration</emphasis> as a reference. This scenario
      would make sense if there is a need to add additional configuration options into running
      configuration used by other components. Usually it is suiteable to use cluster prepared
      configuration as it is.</para>

    </section>

    <section id="testing:yarn:simplified">
      <title>Simplified Testing</title>

      <para>It is perfecly all right to create your tests from scratch and for example create
      the cluster manually and then get the runtime configuration from there. This just needs
      some boilerplate code in your context configuration and unit test lifecycle.</para>

      <para><emphasis>Spring Hadoop</emphasis> adds additional facilities for the testing
      to make all this even easier. </para>

      <programlisting language="java"><![CDATA[@RunWith(SpringJUnit4ClassRunner.class)
public abstract class AbstractYarnClusterTests implements ApplicationContextAware {
  ...
}

@ContextConfiguration(loader=YarnDelegatingSmartContextLoader.class)
@MiniYarnCluster
public class ClusterBaseTestClassTests extends AbstractYarnClusterTests {
  ...
}]]></programlisting>

      <para>Above example shows the <classname>AbstractYarnClusterTests</classname> and how <classname>ClusterBaseTestClassTests</classname>
      is prepared to be aware of a mini cluster. <classname>YarnDelegatingSmartContextLoader</classname> offers same base
      functionality as the default <classname>DelegatingSmartContextLoader</classname> in a spring-test package.
      One additional thing what <classname>YarnDelegatingSmartContextLoader</classname> does is to automatically handle
      running clusters and inject <classname>Configuration</classname> into the application context.</para>

      <programlisting language="java"><![CDATA[@MiniYarnCluster(configName="yarnConfiguration", clusterName="yarnCluster", nodes=1, id="default")]]></programlisting>

      <para>Generally <classname>@MiniYarnCluster</classname> annotation allows you to define injected bean names for
      mini cluster, its Configurations and a number of nodes you like to have in a cluster.</para>

      <para><emphasis>Spring Hadoop Yarn</emphasis> testing is dependant of general facilities of
      <emphasis>Spring Test</emphasis> framework meaning that everything what is cached during the
      test are reuseable withing other tests. One need to understand that if <emphasis>Hadoop</emphasis>
      mini cluster and its <classname>Configuration</classname> is injected into an Application
      Context, caching happens on a mercy of a Spring Testing meaning if a test Application Context is
      cached also mini cluster instance is cached. While caching is always prefered, one needs to
      understant that if tests are expecting vanilla environment to be present, test context should be
      dirtied using <classname>@DirtiesContext</classname> annotation.</para>

      <para><emphasis>Spring Test Context</emphasis> configuration works exactly
      like you'd work with any other <emphasis>Spring Test</emphasis> based tests.
      It defaults on finding xml based config and fall back to Annotation based
      config. For example if one is working with <emphasis>JavaConfig</emphasis>
      a simple static configuration class can be used within the test class.</para>

      <para>For test cases where additional context configuration is not needed
      a simple helper annotation <classname>@MiniYarnClusterTest</classname>
      can be used.</para>

      <programlisting language="java"><![CDATA[@MiniYarnClusterTest
public class ActivatorTests extends AbstractBootYarnClusterTests {
  @Test
  public void testSomething(){
    ...
  }
}]]></programlisting>

      <para>In above example a simple test case was created using annontation
      <emphasis>@MiniYarnClusterTest</emphasis>. Behind a scenes it's using
      junit and prepares a YARN minicluster for you and injects needed configuration
      for you.</para>

      <para>Drawback of using a composed annotation like this is that the
      <emphasis>@Configuration</emphasis> is then applied from an annotation
      class itself and user can't no longer add a static <emphasis>@Configuration</emphasis>
      class in a test class itself and expect Spring to pick it up from
      there which is a normal behaviour in Spring testing support.
      If user wants to use a simple composed annotation and use a
      custom <emphasis>@Configuration</emphasis>, one can simply duplicate
      functionality of this <emphasis>@MiniYarnClusterTest</emphasis> annotation.</para>

      <programlisting language="java"><![CDATA[@Retention(RetentionPolicy.RUNTIME)
@Target(ElementType.TYPE)
@ContextConfiguration(loader=YarnDelegatingSmartContextLoader.class)
@MiniYarnCluster
public @interface CustomMiniYarnClusterTest {

  @Configuration
  public static class Config {
    @Bean
    public String myCustomBean() {
      return "myCustomBean";
    }
  }

}

@RunWith(SpringJUnit4ClassRunner.class)
@CustomMiniYarnClusterTest
public class ComposedAnnotationTests {

  @Autowired
  private ApplicationContext ctx;

  @Test
  public void testBean() {
    assertTrue(ctx.containsBean("myCustomBean"));
  }

}]]></programlisting>

      <para>In above example a custom composed annotation
      <emphasis>@CustomMiniYarnClusterTest</emphasis> was created and
      then used within a test class. This a great way to put your configuration
      is one place and still keep your test class relatively non-verbose.</para>

    </section>

    <section id="testing:yarn:multicontextexample">
      <title>Multi Context Example</title>

      <para>Let's study a proper example of existing Spring Yarn application and how this is tested during
      the build process. Multi Context Example is a simple Spring Yarn based application which simply
      launches Application Master and four Containers and withing those containers a custom code is executed.
      In this case simply a log message is written.</para>

      <para>In real life there are different ways to test whether Hadoop Yarn application execution has been
      succesful or not. The obvious method would be to check the application instance execution status
      reported by Hadoop Yarn. Status of the execution doesn't always tell the whole truth so i.e. if
      application is about to write something into HDFS as an output that could be used to check the
      proper outcome of an execution.</para>

      <para>This example doesn't write anything into HDFS and anyway it would be out of scope of this
      document for obvious reason. It is fairly straightforward to check file content from HDFS. One
      other interesting method is simply to check to application log files that being the Application
      Master and Container logs. Test methods can check exceptions or expected log entries from a log
      files to determine whether test is succesful or not.</para>

      <para>In this chapter we don't go through how Multi Context Example is configured and what it
      actually does, for that read the documentation about the examples. However we go through what
      needs to be done order to test this example application using testing support offered
      by Spring Hadoop.</para>

      <para>In this example we gave instructions to copy library dependencies into Hdfs and then those
      entries were used within resouce localizer to tell Yarn to copy those files into Container
      working directory. During the unit testing when mini cluster is launched there are no files
      present in Hdfs because cluster is initialized from scratch. Furtunalety Spring Hadoop allows you
      to copy files into Hdfs during the localization process from a local file system where Application
      Context is executed. Only thing we need is the actual library files which can be assembled during
      the build process. Spring Hadoop Examples build system rely on Gradle so collecting dependencies
      is an easy task.</para>

      <programlisting language="xml"><![CDATA[<yarn:localresources>
  <yarn:hdfs path="/app/multi-context/*.jar"/>
  <yarn:hdfs path="/lib/*.jar"/>
</yarn:localresources>]]></programlisting>

      <para>Above configuration exists in application-context.xml and appmaster-context.xml files. This
      is a normal application configuration expecting static files already be present in Hdfs. This is
      usually done to minimize latency during the application submission and execution.</para>

      <programlisting language="xml"><![CDATA[<yarn:localresources>
  <yarn:copy src="file:build/dependency-libs/*" dest="/lib/"/>
  <yarn:copy src="file:build/libs/*" dest="/app/multi-context/"/>
  <yarn:hdfs path="/app/multi-context/*.jar"/>
  <yarn:hdfs path="/lib/*.jar"/>
</yarn:localresources>]]></programlisting>

      <para>Above example is from MultiContextTest-context.xml which provides the runtime context
      configuration talking with mini cluster during the test phase.</para>

      <para>When we do context configuration for YarnClient during the testing phase all we need
      to do is to add copy elements which will transfer needed libraries into Hdfs before the actual
      localization process will fire up. When those files are copied into Hdfs running in a mini
      cluster we're basically in a same point if using a real Hadoop cluster with existing files.</para>

      <note><para>Running tests which depends on copying files into Hdfs it is mandatory to use
      build system which is able to prepare these files for you. You can't do this within IDE's
      which have its own ways to execute unit tests.</para></note>

      <para>The complete example of running the test, checking the application execution status and
      finally checking the expected state of log files:</para>

      <programlisting language="java"><![CDATA[@ContextConfiguration(loader=YarnDelegatingSmartContextLoader.class)
@MiniYarnCluster
public class MultiContextTests extends AbstractYarnClusterTests {
  @Test
  @Timed(millis=70000)
  public void testAppSubmission() throws Exception {
    YarnApplicationState state = submitApplicationAndWait();
    assertNotNull(state);
    assertTrue(state.equals(YarnApplicationState.FINISHED));

    File workDir = getYarnCluster().getYarnWorkDir();

    PathMatchingResourcePatternResolver resolver = new PathMatchingResourcePatternResolver();
    String locationPattern = "file:" + workDir.getAbsolutePath() + "/**/*.std*";
    Resource[] resources = resolver.getResources(locationPattern);

    // appmaster and 4 containers should
    // make it 10 log files
    assertThat(resources, notNullValue());
    assertThat(resources.length, is(10));

    for (Resource res : resources) {
      File file = res.getFile();
      if (file.getName().endsWith("stdout")) {
        // there has to be some content in stdout file
        assertThat(file.length(), greaterThan(0l));
        if (file.getName().equals("Container.stdout")) {
          Scanner scanner = new Scanner(file);
          String content = scanner.useDelimiter("\\A").next();
          scanner.close();
          // this is what container will log in stdout
          assertThat(content, containsString("Hello from MultiContextBeanExample"));
        }
      } else if (file.getName().endsWith("stderr")) {
        // can't have anything in stderr files
        assertThat(file.length(), is(0l));
      }
    }
  }
}]]></programlisting>

    </section>

  </section>

  <section id="testing:boot">

    <title>Testing Boot Based Applications</title>

    <para>In previous sections we showed a generic concepts of unit testing
    in <emphasis>Spring Hadoop</emphasis> and <emphasis>Spring YARN</emphasis>.
    We also have a first class support for testing <emphasis>Spring Boot</emphasis>
    based applications made for YARN.</para>

    <para></para>

    <programlisting language="java"><![CDATA[@MiniYarnClusterTest
public class AppTests extends AbstractBootYarnClusterTests {

  @Test
  public void testApp() throws Exception {
    ApplicationInfo info = submitApplicationAndWait(ClientApplication.class, new String[0]);
    assertThat(info.getYarnApplicationState(), is(YarnApplicationState.FINISHED));

    List<Resource> resources = ContainerLogUtils.queryContainerLogs(
      getYarnCluster(), info.getApplicationId());
    assertThat(resources, notNullValue());
    assertThat(resources.size(), is(4));

    for (Resource res : resources) {
      File file = res.getFile();
      String content = ContainerLogUtils.getFileContent(file);
      if (file.getName().endsWith("stdout")) {
        assertThat(file.length(), greaterThan(0l));
        if (file.getName().equals("Container.stdout")) {
          assertThat(content, containsString("Hello from HelloPojo"));
        }
      } else if (file.getName().endsWith("stderr")) {
        assertThat("stderr with content: " + content, file.length(), is(0l));
      }
    }
  }

}]]></programlisting>

    <para>Let’s go through step by step what’s happening in this JUnit class. As
    already mentioned earlier we don’t need any existing or running Hadoop instances,
    instead testing framework from Spring YARN provides an easy way to fire up
    a mini cluster where your tests can be run in an isolated environment.</para>

    <itemizedlist>
      <listitem>
        <para><interfacename>@ContextConfiguration</interfacename> together with
        <classname>YarnDelegatingSmartContextLoader</classname> tells Spring to prepare
        a testing context for a mini cluster. <classname>EmptyConfig</classname> is
        a simple helper class to use if there are no additional configuration for tests.</para>
      </listitem>
      <listitem>
        <para><interfacename>@MiniYarnCluster</interfacename> tells Spring to start
        a Hadoop’s mini cluster having components for <emphasis>HDFS</emphasis>
        and <emphasis>YARN</emphasis>. Hadoop’s configuration from this minicluster is
        automatically injected into your testing context.</para>
      </listitem>
      <listitem>
        <para><interfacename>@MiniYarnClusterTest</interfacename> is basically a replacement
        of <interfacename>@MiniYarnCluster</interfacename> and
        <interfacename>@ContextConfiguration</interfacename> having an empty
        context configuration.</para>
      </listitem>
      <listitem>
        <para><classname>AbstractBootYarnClusterTests</classname> is a class containing
        a lot of base functionality what you need in your tests.</para>
      </listitem>
    </itemizedlist>


    <para>Then it’s time to deploy the application into a running minicluster</para>

    <itemizedlist>
      <listitem>
        <para><literal>submitApplicationAndWait()</literal> method simply runs your
        <classname>ClientApplication</classname> and expects it to an application deployment.
        On default it will wait 60 seconds an application to finish and returns an current state.</para>
      </listitem>
      <listitem>
        <para>We make sure that we have a correct application state</para>
      </listitem>
    </itemizedlist>

    <para>We use <classname>ContainerLogUtils</classname> to find our container
    logs files from a minicluster.</para>

    <itemizedlist>
      <listitem>
        <para>We assert count of a log files</para>
      </listitem>
      <listitem>
        <para>We expect some specified content from log file</para>
      </listitem>
      <listitem>
        <para>We expect stderr files to be empty</para>
      </listitem>
    </itemizedlist>

  </section>


</chapter>
